{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c5f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black Carbon Cross-Method Analysis with Smoothing Techniques\n",
    " \n",
    "This notebook extends the original cross-method analysis by applying three different smoothing techniques to the Aethalometer data before comparison with FTIR and HIPS measurements:\n",
    " \n",
    "1. **Optimized Noise-reduction Algorithm (ONA)**: Adaptive time-averaging based on incremental light attenuation (Î”ATN)\n",
    "2. **Centered Moving Average (CMA)**: Fixed window smoothing that incorporates data points before and after each measurement\n",
    "3. **Double Exponentially Weighted Moving Average (DEMA)**: Weighted smoothing that reduces noise while limiting lag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94800157",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7422f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import os\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from datetime import timedelta\n",
    "from scipy.stats import pearsonr\n",
    "import numba  # Required for ONA algorithm\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc58a69",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading Functions\n",
    " \n",
    "These functions handle loading and preparing data from the Aethalometer, HIPS, and FTIR sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d39c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aethalometer_data(file_path):\n",
    "    \"\"\"Load and prepare Aethalometer data.\"\"\"\n",
    "    print(f\"Loading Aethalometer data from: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Try to use polars for faster loading if available\n",
    "        try:\n",
    "            import polars as pl\n",
    "            print(\"Using Polars for data loading (faster)\")\n",
    "            pl_data = pl.read_csv(file_path)\n",
    "            df = pl_data.to_pandas()\n",
    "        except ImportError:\n",
    "            print(\"Polars not available, using pandas\")\n",
    "            df = pd.read_csv(file_path)\n",
    "    \n",
    "        # Parse date and time columns if they exist\n",
    "        datetime_col = None\n",
    "        if 'Date local (yyyy/MM/dd)' in df.columns and 'Time local (hh:mm:ss)' in df.columns:\n",
    "            # Try multiple formats to handle different date formats\n",
    "            try:\n",
    "                formats_to_try = [\n",
    "                    '%Y-%m-%d %I:%M:%S %p',  # 2022-04-12 9:46:01 AM\n",
    "                    '%Y/%m/%d %H:%M:%S',     # 2022/04/12 09:46:01\n",
    "                    '%Y-%m-%d %H:%M:%S'      # 2022-04-12 09:46:01\n",
    "                ]\n",
    "                \n",
    "                for fmt in formats_to_try:\n",
    "                    try:\n",
    "                        df['datetime_local'] = pd.to_datetime(\n",
    "                            df['Date local (yyyy/MM/dd)'] + ' ' + df['Time local (hh:mm:ss)'],\n",
    "                            format=fmt\n",
    "                        )\n",
    "                        datetime_col = 'datetime_local'\n",
    "                        print(f\"Successfully parsed dates with format: {fmt}\")\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                \n",
    "                if datetime_col is None:\n",
    "                    # If all specific formats fail, try the flexible approach\n",
    "                    df['datetime_local'] = pd.to_datetime(\n",
    "                        df['Date local (yyyy/MM/dd)'] + ' ' + df['Time local (hh:mm:ss)'],\n",
    "                        format='mixed'\n",
    "                    )\n",
    "                    datetime_col = 'datetime_local'\n",
    "                    print(\"Parsed dates using flexible 'mixed' format\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing date/time: {e}\")\n",
    "                print(\"Sample date values:\", df['Date local (yyyy/MM/dd)'].iloc[:5].tolist())\n",
    "                print(\"Sample time values:\", df['Time local (hh:mm:ss)'].iloc[:5].tolist())\n",
    "                return None\n",
    "        elif 'datetime_local' in df.columns:\n",
    "            # Column already exists\n",
    "            df['datetime_local'] = pd.to_datetime(df['datetime_local'])\n",
    "            datetime_col = 'datetime_local'\n",
    "        \n",
    "        # Set index if datetime column is found\n",
    "        if datetime_col:\n",
    "            df.set_index(datetime_col, inplace=True)\n",
    "            df.index = df.index.floor('min')  # Ensure clean 1-min resolution\n",
    "            df.sort_index(inplace=True)\n",
    "        else:\n",
    "            print(\"ERROR: No valid datetime column found\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Loaded {len(df):,} Aethalometer records spanning {df.index.min()} to {df.index.max()}\")\n",
    "        \n",
    "        # Check if Red BCc column exists\n",
    "        if 'Red BCc' in df.columns:\n",
    "            print(f\"Red BCc data available: {df['Red BCc'].describe().round(2)}\")\n",
    "        else:\n",
    "            print(\"WARNING: 'Red BCc' column not found in the dataset!\")\n",
    "            print(\"Available columns:\", df.columns.tolist())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Aethalometer data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0727ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filter_sample_data(db_path):\n",
    "    \"\"\"Load ETAD (HIPS) and FTIR data from SQLite database.\"\"\"\n",
    "    print(f\"Loading ETAD (HIPS) and FTIR filter sample data from: {db_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Load HIPS/FTIR data for the ETAD site\n",
    "        query = \"\"\"\n",
    "        SELECT f.filter_id, \n",
    "               f.sample_date AS SampleDate, \n",
    "               m.ec_ftir AS EC_FTIR,\n",
    "               m.oc_ftir AS OC_FTIR,\n",
    "               m.fabs AS Fabs,\n",
    "               f.site_code AS Site\n",
    "        FROM filters f\n",
    "        JOIN ftir_sample_measurements m USING(filter_id)\n",
    "        WHERE f.site_code = 'ETAD'\n",
    "        ORDER BY f.sample_date;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the query and load into a DataFrame\n",
    "        combined_data = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        combined_data['SampleDate'] = pd.to_datetime(combined_data['SampleDate'])\n",
    "        \n",
    "        # For compatibility with existing code, create separate dataframes for HIPS and FTIR\n",
    "        # Both dataframes contain the same data since they're joined in the database\n",
    "        etad_data = combined_data.copy()\n",
    "        ftir_data = combined_data.copy()\n",
    "        ftir_data.rename(columns={'SampleDate': 'date'}, inplace=True)\n",
    "        \n",
    "        # Display summary\n",
    "        valid_samples_count = etad_data['SampleDate'].notna().sum()\n",
    "        \n",
    "        print(f\"Loaded {len(etad_data)} ETAD samples from database ({valid_samples_count} with valid dates)\")\n",
    "        print(f\"  - All samples have both HIPS and FTIR measurements\")\n",
    "        \n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "        \n",
    "        return etad_data, ftir_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading filter sample data: {e}\")\n",
    "        print(\"Will attempt to create empty dataframes as fallback...\")\n",
    "        etad_data = pd.DataFrame(columns=['filter_id', 'SampleDate', 'Fabs', 'Site'])\n",
    "        ftir_data = pd.DataFrame(columns=['filter_id', 'date', 'EC_FTIR', 'OC_FTIR'])\n",
    "        return etad_data, ftir_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quality_periods(quality_file_path):\n",
    "    \"\"\"Load the quality period classifications from CSV file if available.\"\"\"\n",
    "    print(f\"Looking for quality periods file: {quality_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(quality_file_path):\n",
    "            quality_df = pd.read_csv(quality_file_path)\n",
    "            \n",
    "            # Convert date columns to datetime\n",
    "            if 'start_time' in quality_df.columns:\n",
    "                quality_df['start_time'] = pd.to_datetime(quality_df['start_time'])\n",
    "            if 'end_time' in quality_df.columns:\n",
    "                quality_df['end_time'] = pd.to_datetime(quality_df['end_time'])\n",
    "            \n",
    "            print(f\"Loaded {len(quality_df)} quality-classified periods\")\n",
    "            \n",
    "            # Check if aethalometer_quality column exists\n",
    "            if 'aethalometer_quality' in quality_df.columns:\n",
    "                print(f\"Quality distribution:\")\n",
    "                print(quality_df['aethalometer_quality'].value_counts())\n",
    "            else:\n",
    "                print(\"No 'aethalometer_quality' column found. Available columns:\")\n",
    "                print(quality_df.columns.tolist())\n",
    "            \n",
    "            return quality_df\n",
    "        else:\n",
    "            print(f\"Quality periods file not found: {quality_file_path}\")\n",
    "            print(\"Will need to compute quality periods from scratch.\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading quality periods: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256dc1a",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing Functions\n",
    " \n",
    "These functions identify 'excellent' data quality periods and extract the overlapping measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760892cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_excellent_periods(aethalometer_df, quality_df=None):\n",
    "    \"\"\"\n",
    "    Identify excellent 9am-to-9am periods directly from Aethalometer data\n",
    "    or from a pre-computed quality classification DataFrame.\n",
    "    \"\"\"\n",
    "    if quality_df is not None and 'aethalometer_quality' in quality_df.columns:\n",
    "        print(\"Using pre-computed quality classifications\")\n",
    "        \n",
    "        # Extract excellent periods\n",
    "        excellent_periods = quality_df[quality_df['aethalometer_quality'] == 'Excellent']\n",
    "        print(f\"Found {len(excellent_periods)} excellent periods in quality classifications\")\n",
    "        \n",
    "        return excellent_periods\n",
    "    \n",
    "    print(\"Computing excellent periods from scratch\")\n",
    "    \n",
    "    # Start and end timestamps\n",
    "    start, end = aethalometer_df.index.min(), aethalometer_df.index.max()\n",
    "    \n",
    "    # Create expected full timeline\n",
    "    expected_idx = pd.date_range(start, end, freq='min')\n",
    "    \n",
    "    # Identify missing timestamps\n",
    "    actual_idx = aethalometer_df.index.unique().sort_values()\n",
    "    missing_idx = expected_idx.difference(actual_idx)\n",
    "    \n",
    "    # Create 9am-to-9am periods (matching filter sampling)\n",
    "    # Shift each timestamp to the previous 9am boundary\n",
    "    nine_am_periods = missing_idx.map(lambda ts: \n",
    "        ts.normalize() + pd.Timedelta(hours=9) if ts.hour < 9 \n",
    "        else ts.normalize() + pd.Timedelta(hours=9) + pd.Timedelta(days=1)\n",
    "    )\n",
    "    \n",
    "    # Count missing minutes per 9am-to-9am period\n",
    "    missing_per_period = pd.Series(1, index=nine_am_periods).groupby(level=0).count()\n",
    "    \n",
    "    # Classify excellent periods (â‰¤10 minutes missing)\n",
    "    excellent_periods_idx = missing_per_period[missing_per_period <= 10].index\n",
    "    \n",
    "    # Create a DataFrame with the excellent periods\n",
    "    excellent_periods = pd.DataFrame(index=excellent_periods_idx)\n",
    "    excellent_periods['start_time'] = excellent_periods.index\n",
    "    excellent_periods['end_time'] = excellent_periods['start_time'] + pd.Timedelta(days=1)\n",
    "    excellent_periods['aethalometer_quality'] = 'Excellent'\n",
    "    \n",
    "    print(f\"Computed {len(excellent_periods)} excellent periods from scratch\")\n",
    "    \n",
    "    return excellent_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1246cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_daily_aethalometer_data(aethalometer_df, period_start, period_end, column='Red BCc'):\n",
    "    \"\"\"\n",
    "    Extract and process Aethalometer data for a specific 24-hour period.\n",
    "    Returns summary statistics for the period.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract data for the specific period\n",
    "        period_data = aethalometer_df.loc[period_start:period_end, column].dropna()\n",
    "        \n",
    "        if len(period_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            'period_start': period_start,\n",
    "            'period_end': period_end,\n",
    "            'count': len(period_data),\n",
    "            'mean': period_data.mean(),\n",
    "            'median': period_data.median(),\n",
    "            'min': period_data.min(),\n",
    "            'max': period_data.max(),\n",
    "            'std': period_data.std(),\n",
    "            '25th': period_data.quantile(0.25),\n",
    "            '75th': period_data.quantile(0.75),\n",
    "            'negative_count': (period_data < 0).sum(),\n",
    "            'negative_percent': (period_data < 0).mean() * 100\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data for period {period_start} to {period_end}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlapping_excellent_data(aethalometer_df, filter_df, excellent_periods):\n",
    "    \"\"\"\n",
    "    Find days with overlapping excellent Aethalometer data and filter samples.\n",
    "    Returns a DataFrame with the overlapping periods.\n",
    "    \"\"\"\n",
    "    print(\"\\nFinding overlapping excellent periods with filter samples...\")\n",
    "    \n",
    "    # Print debug info\n",
    "    print(f\"Filter DataFrame columns: {filter_df.columns.tolist()}\")\n",
    "    print(f\"Excellent periods columns: {excellent_periods.columns.tolist()}\")\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    if 'SampleDate' not in filter_df.columns:\n",
    "        print(\"ERROR: SampleDate column not found in filter_df\")\n",
    "        return None\n",
    "    \n",
    "    if 'start_time' not in excellent_periods.columns:\n",
    "        print(\"ERROR: start_time column not found in excellent_periods\")\n",
    "        return None\n",
    "    \n",
    "    # Convert filter sample dates to 9am start time format (24h earlier than collection)\n",
    "    filter_dates = filter_df['SampleDate'].dropna()\n",
    "    print(f\"Filter dates range: {filter_dates.min()} to {filter_dates.max()}\")\n",
    "    print(f\"Total filter dates: {len(filter_dates)}\")\n",
    "    \n",
    "    # Print a few sample filter dates to debug\n",
    "    print(\"Sample filter dates (first 5):\")\n",
    "    for date in filter_dates.head(5):\n",
    "        print(f\"  - {date}\")\n",
    "    \n",
    "    # Convert to 9am start time (24h earlier than collection)\n",
    "    filter_periods = pd.DatetimeIndex([\n",
    "        d.normalize() + pd.Timedelta(hours=9) \n",
    "        for d in filter_dates\n",
    "    ])\n",
    "    \n",
    "    # Get excellent period start times\n",
    "    excellent_starts = excellent_periods['start_time']\n",
    "    print(f\"Excellent periods range: {excellent_starts.min()} to {excellent_starts.max()}\")\n",
    "    print(f\"Total excellent periods: {len(excellent_starts)}\")\n",
    "    \n",
    "    # Print a few sample excellent periods to debug\n",
    "    print(\"Sample excellent period start times (first 5):\")\n",
    "    for date in excellent_starts.head(5):\n",
    "        print(f\"  - {date}\")\n",
    "    \n",
    "    # Find overlap\n",
    "    overlap_periods = pd.DatetimeIndex(filter_periods).intersection(excellent_starts)\n",
    "    \n",
    "    print(f\"Found {len(overlap_periods)} overlapping excellent periods with filter samples\")\n",
    "    \n",
    "    if len(overlap_periods) == 0:\n",
    "        print(\"No overlapping periods found - cannot proceed with comparison.\")\n",
    "        return None\n",
    "    \n",
    "    # Print a few sample overlap periods to debug\n",
    "    print(\"Sample overlap periods (first 5):\")\n",
    "    for date in overlap_periods[:5]:\n",
    "        print(f\"  - {date}\")\n",
    "    \n",
    "    # Create DataFrame with the overlapping periods\n",
    "    overlap_df = pd.DataFrame(index=overlap_periods)\n",
    "    overlap_df['start_time'] = overlap_df.index\n",
    "    overlap_df['end_time'] = overlap_df['start_time'] + pd.Timedelta(days=1)\n",
    "    \n",
    "    # Add filter sample data\n",
    "    # Map each 9am start time back to the filter collection date (24h later)\n",
    "    filter_collection_dates = overlap_df['start_time'] + pd.Timedelta(days=1)\n",
    "    \n",
    "    # Match with filter data - with more debugging and flexible matching\n",
    "    print(\"Adding filter data to overlap DataFrame...\")\n",
    "    filter_columns_added = False\n",
    "    matches_found = 0\n",
    "    \n",
    "    for i, row in overlap_df.iterrows():\n",
    "        # Get the filter collection date (24h after start time)\n",
    "        collection_date = row['start_time'] + pd.Timedelta(days=1)\n",
    "        \n",
    "        # Try exact datetime match first\n",
    "        matching_sample = filter_df[filter_df['SampleDate'] == collection_date]\n",
    "        \n",
    "        # If no exact match, try matching only the date part (ignoring time)\n",
    "        if len(matching_sample) == 0:\n",
    "            # Convert collection_date to date-only for comparison\n",
    "            collection_date_only = collection_date.date()\n",
    "            # Convert filter_df SampleDate to date-only for comparison\n",
    "            matching_sample = filter_df[filter_df['SampleDate'].dt.date == collection_date_only]\n",
    "            \n",
    "            if len(matching_sample) > 0:\n",
    "                print(f\"Found date-only match for {collection_date} -> {matching_sample['SampleDate'].iloc[0]}\")\n",
    "        \n",
    "        if len(matching_sample) > 0:\n",
    "            matches_found += 1\n",
    "            \n",
    "            # Add filter data to the overlap DataFrame\n",
    "            try:\n",
    "                overlap_df.loc[i, 'filter_id'] = matching_sample['filter_id'].iloc[0]\n",
    "                overlap_df.loc[i, 'EC_FTIR'] = matching_sample['EC_FTIR'].iloc[0]\n",
    "                overlap_df.loc[i, 'Fabs'] = matching_sample['Fabs'].iloc[0]\n",
    "                filter_columns_added = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding filter data for {collection_date}: {e}\")\n",
    "            \n",
    "            # Process Aethalometer data for this period\n",
    "            period_end = row['end_time']\n",
    "            aeth_stats = extract_daily_aethalometer_data(aethalometer_df, row['start_time'], period_end)\n",
    "            \n",
    "            if aeth_stats:\n",
    "                for key, value in aeth_stats.items():\n",
    "                    if key not in ['period_start', 'period_end']:\n",
    "                        overlap_df.loc[i, f'aeth_{key}'] = value\n",
    "    \n",
    "    print(f\"Matched {matches_found} out of {len(overlap_df)} overlap periods with filter data\")\n",
    "    \n",
    "    # Check if we successfully added any data\n",
    "    if not filter_columns_added:\n",
    "        print(\"WARNING: No filter data was added to the overlap DataFrame\")\n",
    "        print(\"This could be due to a mismatch in dates or missing columns\")\n",
    "        \n",
    "        # Attempt a more lenient approach - match filter collection dates to the day before excellent periods\n",
    "        print(\"\\nAttempting alternative date matching approach...\")\n",
    "        \n",
    "        # Create a set of all filter collection dates (date only, no time)\n",
    "        filter_dates_set = set(date.date() for date in filter_df['SampleDate'] if not pd.isna(date))\n",
    "        \n",
    "        # Create a new DataFrame to store matches\n",
    "        new_overlap_df = pd.DataFrame()\n",
    "        \n",
    "        # For each filter date, find the corresponding excellent period\n",
    "        matches = 0\n",
    "        for collection_date in filter_dates_set:\n",
    "            # Convert to datetime for easier manipulation\n",
    "            collection_dt = pd.Timestamp(collection_date)\n",
    "            \n",
    "            # Find the excellent period that started 24h before this collection date\n",
    "            start_time = collection_dt.normalize() + pd.Timedelta(hours=9) - pd.Timedelta(days=1)\n",
    "            \n",
    "            # Check if this start time is in excellent_periods\n",
    "            if start_time in excellent_starts.values:\n",
    "                matches += 1\n",
    "                \n",
    "                # Get filter data\n",
    "                matching_filter = filter_df[filter_df['SampleDate'].dt.date == collection_date]\n",
    "                \n",
    "                if len(matching_filter) > 0:\n",
    "                    # Create a new row for this match\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'start_time': [start_time],\n",
    "                        'end_time': [start_time + pd.Timedelta(days=1)],\n",
    "                        'filter_id': [matching_filter['filter_id'].iloc[0]],\n",
    "                        'EC_FTIR': [matching_filter['EC_FTIR'].iloc[0]],\n",
    "                        'Fabs': [matching_filter['Fabs'].iloc[0]]\n",
    "                    })\n",
    "                    \n",
    "                    # Add Aethalometer data\n",
    "                    aeth_stats = extract_daily_aethalometer_data(\n",
    "                        aethalometer_df, start_time, start_time + pd.Timedelta(days=1)\n",
    "                    )\n",
    "                    \n",
    "                    if aeth_stats:\n",
    "                        for key, value in aeth_stats.items():\n",
    "                            if key not in ['period_start', 'period_end']:\n",
    "                                new_row[f'aeth_{key}'] = value\n",
    "                    \n",
    "                    # Append to our new DataFrame\n",
    "                    new_overlap_df = pd.concat([new_overlap_df, new_row], ignore_index=True)\n",
    "        \n",
    "        if matches > 0:\n",
    "            print(f\"Alternative approach found {matches} matches\")\n",
    "            if len(new_overlap_df) > 0:\n",
    "                print(f\"Successfully created alternate overlap DataFrame with {len(new_overlap_df)} rows\")\n",
    "                print(f\"Columns: {new_overlap_df.columns.tolist()}\")\n",
    "                return new_overlap_df\n",
    "    \n",
    "    # Check which columns exist before dropping\n",
    "    print(f\"Overlap DataFrame columns: {overlap_df.columns.tolist()}\")\n",
    "    \n",
    "    # Define required columns\n",
    "    required_cols = ['filter_id', 'aeth_mean']\n",
    "    missing_cols = [col for col in required_cols if col not in overlap_df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"WARNING: Missing required columns: {missing_cols}\")\n",
    "        print(\"Keeping all rows for now\")\n",
    "    else:\n",
    "        # Drop any rows without filter or Aethalometer data\n",
    "        print(\"Dropping rows with missing filter or Aethalometer data...\")\n",
    "        orig_len = len(overlap_df)\n",
    "        print(f\"Dropped {orig_len - len(overlap_df)} incomplete rows\")\n",
    "    \n",
    "    print(f\"Final count of complete overlapping periods: {len(overlap_df)}\")\n",
    "    \n",
    "    return overlap_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b417cd3",
   "metadata": {},
   "source": [
    "## Step 3: Smoothing Algorithm Implementations\n",
    " \n",
    "Here we implement the three smoothing algorithms to apply to Aethalometer data prior to comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f7577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Optimized Noise-reduction Algorithm (ONA)\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def _numba_nanmean(arr_slice):\n",
    "    \"\"\"\n",
    "    Numba-compatible nanmean function.\n",
    "    \"\"\"\n",
    "    finite_sum = 0.0\n",
    "    finite_count = 0\n",
    "    for x in arr_slice:\n",
    "        if not np.isnan(x): # np.isnan is Numba compatible\n",
    "            finite_sum += x\n",
    "            finite_count += 1\n",
    "    if finite_count == 0:\n",
    "        return np.nan\n",
    "    return finite_sum / finite_count\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def process_ona_segment_numba_core(\n",
    "    bc_col_np: np.ndarray,\n",
    "    atn_col_np: np.ndarray,\n",
    "    delta_atn_min: float,\n",
    "    num_rows: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Core ONA algorithm implemented for Numba.\n",
    "    Operates on NumPy arrays for a single data segment.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays\n",
    "    smoothed_col_np = bc_col_np.copy() # Start with original BC values\n",
    "    points_col_np = np.ones(num_rows, dtype=np.int64) # Default points averaged is 1\n",
    "\n",
    "    # If only one point (or none) in the segment, it's effectively averaged with itself.\n",
    "    if num_rows <= 1:\n",
    "        return smoothed_col_np, points_col_np\n",
    "\n",
    "    j = 0 # Current starting index of the averaging window (0-based)\n",
    "    while j < num_rows:\n",
    "        cur_atn = atn_col_np[j]\n",
    "\n",
    "        # Find the end of the window (exclusive index).\n",
    "        # The window includes point j. All subsequent points k in the window\n",
    "        # must satisfy: atn_col_np[k] <= cur_atn + delta_atn_min\n",
    "        window_end_exclusive = j + 1\n",
    "        while window_end_exclusive < num_rows:\n",
    "            if atn_col_np[window_end_exclusive] > cur_atn + delta_atn_min:\n",
    "                break # End of window found\n",
    "            window_end_exclusive += 1\n",
    "        \n",
    "        # The current window for averaging is from index j to window_end_exclusive-1\n",
    "        slice_bc_np = bc_col_np[j:window_end_exclusive]\n",
    "        \n",
    "        avg_bc = _numba_nanmean(slice_bc_np)\n",
    "        num_points_in_window = window_end_exclusive - j # Length of the slice\n",
    "\n",
    "        # Apply the averaged BC value and points count to all rows in the window\n",
    "        for k_idx in range(j, window_end_exclusive):\n",
    "            smoothed_col_np[k_idx] = avg_bc\n",
    "            points_col_np[k_idx] = num_points_in_window\n",
    "            \n",
    "        # Move to the start of the next potential window\n",
    "        j = window_end_exclusive\n",
    "            \n",
    "    return smoothed_col_np, points_col_np\n",
    "\n",
    "def process_ona_segment_numba_wrapper(\n",
    "    pdf: pd.DataFrame,\n",
    "    bc_col: str,\n",
    "    atn_col: str,\n",
    "    points_col: str,\n",
    "    smoothed_col: str,\n",
    "    delta_atn_min: float\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Wrapper to run the ONA algorithm on one pandas segment using the Numba core.\n",
    "    Returns the pandas DataFrame with updated points_col and smoothed_col columns.\n",
    "    \"\"\"\n",
    "    # Initialize the columns with default values.\n",
    "    if points_col not in pdf.columns:\n",
    "        pdf[points_col] = 1\n",
    "    else:\n",
    "        pdf.loc[:, points_col] = 1 # Use .loc to avoid SettingWithCopyWarning\n",
    "\n",
    "    if smoothed_col not in pdf.columns:\n",
    "        pdf[smoothed_col] = pdf[bc_col].copy()\n",
    "    else:\n",
    "        pdf.loc[:, smoothed_col] = pdf[bc_col].copy()\n",
    "\n",
    "    if len(pdf) <= 1:\n",
    "        # If the segment is very short, ensure original values and 1 point averaged is returned\n",
    "        pdf.loc[:, smoothed_col] = pdf[bc_col].copy()\n",
    "        pdf.loc[:, points_col] = 1\n",
    "        return pdf\n",
    "\n",
    "    # Convert relevant pandas Series to NumPy arrays for Numba\n",
    "    bc_col_np = pdf[bc_col].to_numpy(dtype=np.float64, na_value=np.nan)\n",
    "    atn_col_np = pdf[atn_col].to_numpy(dtype=np.float64, na_value=np.nan)\n",
    "    \n",
    "    num_rows = len(pdf)\n",
    "\n",
    "    # Call the Numba-optimized core function\n",
    "    smoothed_result_np, points_result_np = process_ona_segment_numba_core(\n",
    "        bc_col_np, atn_col_np, delta_atn_min, num_rows\n",
    "    )\n",
    "\n",
    "    # Assign results back to the pandas DataFrame using .loc to ensure modification\n",
    "    pdf.loc[:, smoothed_col] = smoothed_result_np\n",
    "    pdf.loc[:, points_col] = points_result_np\n",
    "    \n",
    "    return pdf\n",
    "\n",
    "def apply_ona(data, wavelength=\"Red\", delta_atn_min=0.05):\n",
    "    \"\"\"\n",
    "    Apply the Optimized Noise-reduction Algorithm to Aethalometer data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        DataFrame with Aethalometer data\n",
    "    wavelength : str\n",
    "        Which wavelength to process ('UV', 'Blue', 'Green', 'Red', 'IR')\n",
    "    delta_atn_min : float\n",
    "        Minimum change in attenuation to define a window\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    data_processed : pandas.DataFrame\n",
    "        DataFrame with original data plus additional columns for ONA processed BC\n",
    "    \"\"\"\n",
    "    # Column names\n",
    "    bc_col = f\"{wavelength} BCc\"\n",
    "    atn_col = f\"{wavelength} ATN1\"\n",
    "    points_col = f\"{wavelength}_points_averaged\"\n",
    "    smoothed_col = f\"{wavelength}_BC_ONA\"\n",
    "\n",
    "    # Check if required columns exist\n",
    "    if atn_col not in data.columns or bc_col not in data.columns:\n",
    "        print(f\"Warning: Required columns ({atn_col}, {bc_col}) not found for ONA on wavelength {wavelength}. Skipping ONA.\")\n",
    "        result_df = data.copy()\n",
    "        if points_col not in result_df.columns:\n",
    "            result_df[points_col] = 1\n",
    "        if smoothed_col not in result_df.columns:\n",
    "            if bc_col in result_df.columns:\n",
    "                result_df[smoothed_col] = result_df[bc_col]\n",
    "            else:\n",
    "                result_df[smoothed_col] = np.nan\n",
    "        return result_df\n",
    "\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Calculate absolute ATN differences to identify filter changes\n",
    "    df['Î”ATN'] = df[atn_col].diff().abs()\n",
    "    \n",
    "    # A new segment starts if Î”ATN > 30 OR if Î”ATN is null (first row)\n",
    "    df['segment_id'] = ((df['Î”ATN'] > 30) | df['Î”ATN'].isna()).astype(int).cumsum()\n",
    "    \n",
    "    actual_filter_changes = df[df['Î”ATN'] > 30].shape[0]\n",
    "    print(f\"Number of actual filter changes (Î”ATN > 30) detected for {wavelength}: {actual_filter_changes}\")\n",
    "    \n",
    "    # Initialize output columns\n",
    "    df[points_col] = 1\n",
    "    df[smoothed_col] = df[bc_col].copy()\n",
    "    \n",
    "    # Process each segment\n",
    "    segments = df.groupby('segment_id')\n",
    "    processed_segments = []\n",
    "    \n",
    "    for seg_id, segment in segments:\n",
    "        # Sort by index within segment to ensure time order\n",
    "        segment = segment.sort_index()\n",
    "        \n",
    "        # Process the segment\n",
    "        processed_segment = process_ona_segment_numba_wrapper(\n",
    "            segment, bc_col, atn_col, points_col, smoothed_col, delta_atn_min\n",
    "        )\n",
    "        \n",
    "        processed_segments.append(processed_segment)\n",
    "    \n",
    "    # Combine processed segments\n",
    "    if processed_segments:\n",
    "        result_df = pd.concat(processed_segments)\n",
    "        # Restore original sort order\n",
    "        result_df = result_df.sort_index()\n",
    "        \n",
    "        # Drop temporary columns\n",
    "        result_df = result_df.drop(['Î”ATN', 'segment_id'], axis=1)\n",
    "    else:\n",
    "        # Fallback if no segments were processed\n",
    "        result_df = df.drop(['Î”ATN', 'segment_id'], axis=1)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d33ad",
   "metadata": {},
   "source": [
    "# Implementation of Centered Moving Average (CMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80550e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cma(data, wavelength='Red', window_size=None):\n",
    "    \"\"\"\n",
    "    Apply the Centered Moving Average algorithm to Aethalometer data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        DataFrame containing Aethalometer data\n",
    "    wavelength : str\n",
    "        Which wavelength to process ('UV', 'Blue', 'Green', 'Red', 'IR')\n",
    "    window_size : int or None\n",
    "        Size of the moving average window (must be odd). If None, \n",
    "        will use a default based on the data's timebase\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    data_smoothed : pandas.DataFrame\n",
    "        DataFrame with the original data plus additional columns for smoothed BC\n",
    "    \"\"\"\n",
    "    # Create a copy of the input dataframe\n",
    "    data_smoothed = data.copy()\n",
    "    \n",
    "    # Identify the column for BC values based on wavelength\n",
    "    bc_col = f\"{wavelength} BCc\"\n",
    "    smoothed_bc_col = f\"{wavelength}_BC_CMA\"\n",
    "    \n",
    "    # Determine window size if not specified\n",
    "    if window_size is None:\n",
    "        if 'Timebase (s)' in data_smoothed.columns:\n",
    "            timebase = data_smoothed['Timebase (s)'].iloc[0]\n",
    "            if timebase == 1:\n",
    "                window_size = 11  # 11 seconds for 1-second data\n",
    "            elif timebase == 5:\n",
    "                window_size = 5   # 25 seconds for 5-second data\n",
    "            elif timebase == 60:\n",
    "                window_size = 3   # 3 minutes for 1-minute data\n",
    "            else:\n",
    "                window_size = 5   # Default for other timebases\n",
    "        else:\n",
    "            window_size = 5       # Default if timebase is unknown\n",
    "    \n",
    "    # Make sure window_size is odd\n",
    "    if window_size % 2 == 0:\n",
    "        window_size += 1\n",
    "    \n",
    "    print(f\"Using window size of {window_size} for CMA on {wavelength}\")\n",
    "    \n",
    "    # Apply rolling mean with center=True\n",
    "    if bc_col in data_smoothed.columns:\n",
    "        data_smoothed[smoothed_bc_col] = data_smoothed[bc_col].rolling(\n",
    "            window=window_size, center=True, min_periods=1\n",
    "        ).mean()\n",
    "    else:\n",
    "        print(f\"Warning: {bc_col} not found in data, cannot apply CMA\")\n",
    "        data_smoothed[smoothed_bc_col] = np.nan\n",
    "    \n",
    "    return data_smoothed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6dbd37",
   "metadata": {},
   "source": [
    "# Implementation of Double Exponentially Weighted Moving Average (DEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fad5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dema(data, wavelength='Red', alpha=None):\n",
    "    \"\"\"\n",
    "    Apply the Double Exponentially Weighted Moving Average algorithm\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        DataFrame containing Aethalometer data\n",
    "    wavelength : str\n",
    "        Which wavelength to process ('UV', 'Blue', 'Green', 'Red', 'IR')\n",
    "    alpha : float\n",
    "        Smoothing parameter (between 0 and 1)\n",
    "        For 60s data, 0.125 approximates a 15-minute smoothing window\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    data_smoothed : pandas.DataFrame\n",
    "        DataFrame with the original data plus additional columns for smoothed BC\n",
    "    \"\"\"\n",
    "    # Create a copy of the input dataframe\n",
    "    data_smoothed = data.copy()\n",
    "    \n",
    "    # Identify the column for BC values based on wavelength\n",
    "    bc_col = f\"{wavelength} BCc\"\n",
    "    ema_col = f\"{wavelength}_EMA\"\n",
    "    dema_col = f\"{wavelength}_BC_DEMA\"\n",
    "    \n",
    "    # Set the smoothing parameter based on timebase if not explicitly provided\n",
    "    if 'Timebase (s)' in data_smoothed.columns:\n",
    "        timebase = data_smoothed['Timebase (s)'].iloc[0]\n",
    "        if alpha is None:\n",
    "            # Use formula 2/(N+1) where N is the desired smoothing period\n",
    "            if timebase == 1:\n",
    "                # Default to approximate 5-minute window for 1-second data\n",
    "                N = 300 / timebase\n",
    "            elif timebase == 5:\n",
    "                # Default to approximate 5-minute window for 5-second data\n",
    "                N = 300 / timebase\n",
    "            elif timebase == 60:\n",
    "                # Default to approximate 15-minute window for 60-second data\n",
    "                N = 900 / timebase\n",
    "            else:\n",
    "                N = 15  # Default for other timebases\n",
    "                \n",
    "            alpha = 2 / (N + 1)\n",
    "    else:\n",
    "        # Default alpha if timebase is unknown\n",
    "        if alpha is None:\n",
    "            alpha = 0.125\n",
    "    \n",
    "    print(f\"Using alpha of {alpha:.4f} for DEMA on {wavelength}\")\n",
    "    \n",
    "    if bc_col in data_smoothed.columns:\n",
    "        # First EMA calculation\n",
    "        data_smoothed[ema_col] = data_smoothed[bc_col].ewm(alpha=alpha, adjust=False).mean()\n",
    "        \n",
    "        # Second EMA calculation (EMA of EMA)\n",
    "        ema_of_ema = data_smoothed[ema_col].ewm(alpha=alpha, adjust=False).mean()\n",
    "        \n",
    "        # Calculate DEMA: (2 * EMA) - EMA(EMA)\n",
    "        data_smoothed[dema_col] = 2 * data_smoothed[ema_col] - ema_of_ema\n",
    "    else:\n",
    "        print(f\"Warning: {bc_col} not found in data, cannot apply DEMA\")\n",
    "        data_smoothed[ema_col] = np.nan\n",
    "        data_smoothed[dema_col] = np.nan\n",
    "    \n",
    "    return data_smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6def045",
   "metadata": {},
   "source": [
    "## Step 4: Function to Apply All Smoothing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c15343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_all_smoothing_methods(aethalometer_df, wavelength=\"Red\"):\n",
    "    \"\"\"\n",
    "    Apply all three smoothing methods to aethalometer data for a specific wavelength.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    aethalometer_df : pandas.DataFrame\n",
    "        DataFrame with aethalometer data\n",
    "    wavelength : str\n",
    "        Wavelength to process ('UV', 'Blue', 'Green', 'Red', 'IR')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    smoothed_data : dict\n",
    "        Dictionary with keys 'raw', 'ona', 'cma', 'dema' containing the processed dataframes\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying smoothing methods to {wavelength} wavelength data...\")\n",
    "    \n",
    "    # Raw data (unprocessed)\n",
    "    raw_data = aethalometer_df.copy()\n",
    "    \n",
    "    # Apply ONA\n",
    "    print(f\"\\nApplying ONA to {wavelength} wavelength data...\")\n",
    "    ona_data = apply_ona(aethalometer_df, wavelength)\n",
    "    \n",
    "    # Apply CMA\n",
    "    print(f\"\\nApplying CMA to {wavelength} wavelength data...\")\n",
    "    cma_data = apply_cma(aethalometer_df, wavelength)\n",
    "    \n",
    "    # Apply DEMA\n",
    "    print(f\"\\nApplying DEMA to {wavelength} wavelength data...\")\n",
    "    dema_data = apply_dema(aethalometer_df, wavelength)\n",
    "    \n",
    "    return {\n",
    "        'raw': raw_data,\n",
    "        'ona': ona_data,\n",
    "        'cma': cma_data,\n",
    "        'dema': dema_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a635584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_smoothed_aethalometer_data(smoothed_data, method, period_start, period_end, wavelength=\"Red\"):\n",
    "    \"\"\"\n",
    "    Extract and process smoothed Aethalometer data for a specific 24-hour period.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    smoothed_data : dict\n",
    "        Dictionary with keys 'raw', 'ona', 'cma', 'dema' containing the processed dataframes\n",
    "    method : str\n",
    "        Smoothing method ('raw', 'ona', 'cma', 'dema')\n",
    "    period_start, period_end : datetime\n",
    "        Start and end of the 24-hour period\n",
    "    wavelength : str\n",
    "        Wavelength to process ('UV', 'Blue', 'Green', 'Red', 'IR')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    stats : dict\n",
    "        Dictionary with summary statistics for the period\n",
    "    \"\"\"\n",
    "    # Select the column based on smoothing method\n",
    "    if method == 'raw':\n",
    "        column = f\"{wavelength} BCc\"\n",
    "        df = smoothed_data['raw']\n",
    "    elif method == 'ona':\n",
    "        column = f\"{wavelength}_BC_ONA\"\n",
    "        df = smoothed_data['ona']\n",
    "    elif method == 'cma':\n",
    "        column = f\"{wavelength}_BC_CMA\"\n",
    "        df = smoothed_data['cma']\n",
    "    elif method == 'dema':\n",
    "        column = f\"{wavelength}_BC_DEMA\"\n",
    "        df = smoothed_data['dema']\n",
    "    else:\n",
    "        print(f\"Invalid method: {method}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Extract data for the specific period\n",
    "        period_data = df.loc[period_start:period_end, column].dropna()\n",
    "        \n",
    "        if len(period_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            'period_start': period_start,\n",
    "            'period_end': period_end,\n",
    "            'count': len(period_data),\n",
    "            'mean': period_data.mean(),\n",
    "            'median': period_data.median(),\n",
    "            'min': period_data.min(),\n",
    "            'max': period_data.max(),\n",
    "            'std': period_data.std(),\n",
    "            '25th': period_data.quantile(0.25),\n",
    "            '75th': period_data.quantile(0.75),\n",
    "            'negative_count': (period_data < 0).sum(),\n",
    "            'negative_percent': (period_data < 0).mean() * 100\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {method} data for period {period_start} to {period_end}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ad379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overlap_df_with_all_methods(overlap_df_raw, smoothed_data, wavelength=\"Red\"):\n",
    "    \"\"\"\n",
    "    Add columns for all smoothing methods to the overlap DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    overlap_df_raw : pandas.DataFrame\n",
    "        DataFrame with periods that overlap between Aethalometer and filter samples\n",
    "    smoothed_data : dict\n",
    "        Dictionary with keys 'raw', 'ona', 'cma', 'dema' containing the processed dataframes\n",
    "    wavelength : str\n",
    "        Wavelength to process ('UV', 'Blue', 'Green', 'Red', 'IR')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    overlap_df_all : pandas.DataFrame\n",
    "        DataFrame with additional columns for all smoothing methods\n",
    "    \"\"\"\n",
    "    # Create a copy of the raw overlap DataFrame\n",
    "    overlap_df_all = overlap_df_raw.copy()\n",
    "    \n",
    "    # Methods to process\n",
    "    methods = ['raw', 'ona', 'cma', 'dema']\n",
    "    \n",
    "    # Process each period for each method\n",
    "    for i, row in overlap_df_all.iterrows():\n",
    "        period_start = row['start_time']\n",
    "        period_end = row['end_time']\n",
    "        \n",
    "        for method in methods:\n",
    "            # Skip 'raw' method if we already have the statistics\n",
    "            if method == 'raw' and 'aeth_mean' in overlap_df_all.columns:\n",
    "                continue\n",
    "                \n",
    "            # Extract statistics for this method\n",
    "            stats = extract_smoothed_aethalometer_data(\n",
    "                smoothed_data, method, period_start, period_end, wavelength\n",
    "            )\n",
    "            \n",
    "            if stats:\n",
    "                for key, value in stats.items():\n",
    "                    if key not in ['period_start', 'period_end']:\n",
    "                        # Use method-specific column name (except for raw which keeps 'aeth_')\n",
    "                        prefix = 'aeth_' if method == 'raw' else f'{method}_'\n",
    "                        overlap_df_all.loc[i, f'{prefix}{key}'] = value\n",
    "    \n",
    "    return overlap_df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8335fd",
   "metadata": {},
   "source": [
    "## Step 6: Enhanced Visualization Functions for Comparison with Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd529ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bc_scatter_all_methods(overlap_df, y_param='EC_FTIR', wavelength=\"Red\", title_prefix=None):\n",
    "    \"\"\"\n",
    "    Create scatter plots comparing all smoothing methods with FTIR EC or HIPS Fabs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    overlap_df : pandas.DataFrame\n",
    "        DataFrame with overlapping periods including all smoothing methods\n",
    "    y_param : str\n",
    "        Filter parameter to compare against ('EC_FTIR' or 'Fabs')\n",
    "    wavelength : str\n",
    "        Wavelength used for BC data\n",
    "    title_prefix : str or None\n",
    "        Optional prefix for plot title\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        Figure with the plots\n",
    "    \"\"\"\n",
    "    # Method column mappings\n",
    "    method_columns = {\n",
    "        'Raw': 'aeth_mean',\n",
    "        'ONA': 'ona_mean',\n",
    "        'CMA': 'cma_mean',\n",
    "        'DEMA': 'dema_mean'\n",
    "    }\n",
    "    \n",
    "    # Method colors and markers\n",
    "    method_styles = {\n",
    "        'Raw': {'color': 'black', 'marker': 'o', 'alpha': 0.5},\n",
    "        'ONA': {'color': 'blue', 'marker': 's', 'alpha': 0.7},\n",
    "        'CMA': {'color': 'red', 'marker': '^', 'alpha': 0.7},\n",
    "        'DEMA': {'color': 'green', 'marker': 'D', 'alpha': 0.7}\n",
    "    }\n",
    "    \n",
    "    # Parameter labels\n",
    "    param_labels = {\n",
    "        'EC_FTIR': 'FTIR EC (Î¼g/mÂ³)',\n",
    "        'Fabs': 'HIPS Fabs'\n",
    "    }\n",
    "    \n",
    "    # Create figure with 2x2 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Title for the figure\n",
    "    if title_prefix is None:\n",
    "        title_prefix = f\"{wavelength} BCc vs\"\n",
    "    fig.suptitle(f\"{title_prefix} {param_labels.get(y_param, y_param)}\\nComparison of Smoothing Methods\", fontsize=16)\n",
    "    \n",
    "    # Process each method\n",
    "    regression_results = {}\n",
    "    for i, (method_name, col_name) in enumerate(method_columns.items()):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Skip if column not available\n",
    "        if col_name not in overlap_df.columns or y_param not in overlap_df.columns:\n",
    "            ax.text(0.5, 0.5, f\"Data not available for {method_name}\", \n",
    "                    ha='center', va='center', transform=ax.transAxes)\n",
    "            continue\n",
    "        \n",
    "        # Prepare data\n",
    "        x = overlap_df[col_name] / 1000  # Convert from ng/mÂ³ to Î¼g/mÂ³\n",
    "        y = overlap_df[y_param]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        style = method_styles[method_name]\n",
    "        scatter = ax.scatter(x, y, s=80, **style)\n",
    "        \n",
    "        # Calculate and add trendline\n",
    "        mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "        if mask.sum() > 1:  # Need at least 2 valid points for regression\n",
    "            slope, intercept = np.polyfit(x[mask], y[mask], 1)\n",
    "            x_line = np.linspace(x.min(), x.max(), 100)\n",
    "            y_line = slope * x_line + intercept\n",
    "            ax.plot(x_line, y_line, 'r--', linewidth=2)\n",
    "            \n",
    "            # Add regression equation and RÂ² to plot\n",
    "            r_value = pearsonr(x[mask], y[mask])[0]\n",
    "            r_squared = r_value**2\n",
    "            \n",
    "            equation = f\"y = {slope:.3f}x + {intercept:.3f}\"\n",
    "            r2_text = f\"RÂ² = {r_squared:.3f}\"\n",
    "            \n",
    "            ax.annotate(\n",
    "                equation + '\\n' + r2_text,\n",
    "                xy=(0.05, 0.95),\n",
    "                xycoords='axes fraction',\n",
    "                fontsize=12,\n",
    "                ha='left',\n",
    "                va='top',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8)\n",
    "            )\n",
    "            \n",
    "            # Store regression results\n",
    "            regression_results[method_name] = {\n",
    "                'slope': slope,\n",
    "                'intercept': intercept,\n",
    "                'r_squared': r_squared,\n",
    "                'data_points': mask.sum()\n",
    "            }\n",
    "        \n",
    "        # Format subplot\n",
    "        ax.set_title(f\"{method_name} Processing\", fontsize=14)\n",
    "        if i >= 2:  # Only add x-label to bottom row\n",
    "            ax.set_xlabel(f'Aethalometer {wavelength} BC (Î¼g/mÂ³)', fontsize=12)\n",
    "        if i % 2 == 0:  # Only add y-label to left column\n",
    "            ax.set_ylabel(param_labels.get(y_param, y_param), fontsize=12)\n",
    "        \n",
    "        # Make sure axes start at 0 if all data is positive\n",
    "        if x.min() >= 0 and y.min() >= 0:\n",
    "            ax.set_xlim(left=0)\n",
    "            ax.set_ylim(bottom=0)\n",
    "        \n",
    "        # Add grid\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add 1:1 line if units are comparable\n",
    "        if y_param == 'EC_FTIR':\n",
    "            max_val = max(x.max(), y.max()) * 1.1\n",
    "            ax.plot([0, max_val], [0, max_val], 'k--', alpha=0.3, linewidth=1, label='1:1 Line')\n",
    "            ax.legend()\n",
    "    \n",
    "    # Add a summary table in a text box\n",
    "    if regression_results:\n",
    "        table_text = \"Regression Results:\\n\"\n",
    "        table_text += \"-\" * 60 + \"\\n\"\n",
    "        table_text += f\"{'Method':<10} {'Slope':<10} {'Intercept':<10} {'RÂ²':<10} {'N':<10}\\n\"\n",
    "        table_text += \"-\" * 60 + \"\\n\"\n",
    "        \n",
    "        for method, results in regression_results.items():\n",
    "            table_text += f\"{method:<10} {results['slope']:<10.3f} {results['intercept']:<10.3f} \"\n",
    "            table_text += f\"{results['r_squared']:<10.3f} {results['data_points']:<10}\\n\"\n",
    "        \n",
    "        fig.text(0.5, 0.01, table_text, ha='center', va='bottom', \n",
    "                fontsize=10, family='monospace', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9, bottom=0.15)  # Adjust for suptitle and table\n",
    "    \n",
    "    return fig, regression_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_method_comparison_all_smoothing(overlap_df, wavelength=\"Red\"):\n",
    "    \"\"\"\n",
    "    Create a comprehensive comparison plot of all smoothing methods against both FTIR and HIPS.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    overlap_df : pandas.DataFrame\n",
    "        DataFrame with overlapping periods including all smoothing methods\n",
    "    wavelength : str\n",
    "        Wavelength used for BC data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        Figure with the plots\n",
    "    \"\"\"\n",
    "    # Method column mappings\n",
    "    method_columns = {\n",
    "        'Raw': 'aeth_mean',\n",
    "        'ONA': 'ona_mean',\n",
    "        'CMA': 'cma_mean',\n",
    "        'DEMA': 'dema_mean'\n",
    "    }\n",
    "    \n",
    "    # Method colors and markers\n",
    "    method_styles = {\n",
    "        'Raw': {'color': 'black', 'marker': 'o', 'alpha': 0.5},\n",
    "        'ONA': {'color': 'blue', 'marker': 's', 'alpha': 0.7},\n",
    "        'CMA': {'color': 'red', 'marker': '^', 'alpha': 0.7},\n",
    "        'DEMA': {'color': 'green', 'marker': 'D', 'alpha': 0.7}\n",
    "    }\n",
    "    \n",
    "    # Create figure with 2 subplots (one for FTIR, one for HIPS)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # Plot titles\n",
    "    axes[0].set_title(f\"Comparison with FTIR EC\\n({wavelength} Wavelength, {len(overlap_df)} periods)\", fontsize=14)\n",
    "    axes[1].set_title(f\"Comparison with HIPS Fabs\\n({wavelength} Wavelength, {len(overlap_df)} periods)\", fontsize=14)\n",
    "    \n",
    "    # Process each method for FTIR EC (left plot)\n",
    "    regression_results_ftir = {}\n",
    "    for method_name, col_name in method_columns.items():\n",
    "        if col_name in overlap_df.columns and 'EC_FTIR' in overlap_df.columns:\n",
    "            # Prepare data\n",
    "            x = overlap_df[col_name] / 1000  # Convert from ng/mÂ³ to Î¼g/mÂ³\n",
    "            y = overlap_df['EC_FTIR']\n",
    "            \n",
    "            # Create scatter plot\n",
    "            style = method_styles[method_name]\n",
    "            scatter = axes[0].scatter(x, y, s=80, label=method_name, **style)\n",
    "            \n",
    "            # Calculate and add trendline\n",
    "            mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "            if mask.sum() > 1:  # Need at least 2 valid points for regression\n",
    "                slope, intercept = np.polyfit(x[mask], y[mask], 1)\n",
    "                x_line = np.linspace(x.min(), x.max(), 100)\n",
    "                y_line = slope * x_line + intercept\n",
    "                axes[0].plot(x_line, y_line, color=style['color'], linestyle='--', linewidth=2)\n",
    "                \n",
    "                # Store regression results\n",
    "                r_value = pearsonr(x[mask], y[mask])[0]\n",
    "                r_squared = r_value**2\n",
    "                regression_results_ftir[method_name] = {\n",
    "                    'slope': slope,\n",
    "                    'intercept': intercept,\n",
    "                    'r_squared': r_squared,\n",
    "                    'data_points': mask.sum()\n",
    "                }\n",
    "    \n",
    "    # Add 1:1 line to FTIR plot\n",
    "    max_val_ftir = axes[0].get_xlim()[1]\n",
    "    axes[0].plot([0, max_val_ftir], [0, max_val_ftir], 'k--', alpha=0.3, linewidth=1, label='1:1 Line')\n",
    "    \n",
    "    # Process each method for HIPS Fabs (right plot)\n",
    "    regression_results_hips = {}\n",
    "    for method_name, col_name in method_columns.items():\n",
    "        if col_name in overlap_df.columns and 'Fabs' in overlap_df.columns:\n",
    "            # Prepare data\n",
    "            x = overlap_df[col_name] / 1000  # Convert from ng/mÂ³ to Î¼g/mÂ³\n",
    "            y = overlap_df['Fabs']\n",
    "            \n",
    "            # Create scatter plot\n",
    "            style = method_styles[method_name]\n",
    "            scatter = axes[1].scatter(x, y, s=80, label=method_name, **style)\n",
    "            \n",
    "            # Calculate and add trendline\n",
    "            mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "            if mask.sum() > 1:  # Need at least 2 valid points for regression\n",
    "                slope, intercept = np.polyfit(x[mask], y[mask], 1)\n",
    "                x_line = np.linspace(x.min(), x.max(), 100)\n",
    "                y_line = slope * x_line + intercept\n",
    "                axes[1].plot(x_line, y_line, color=style['color'], linestyle='--', linewidth=2)\n",
    "                \n",
    "                # Store regression results\n",
    "                r_value = pearsonr(x[mask], y[mask])[0]\n",
    "                r_squared = r_value**2\n",
    "                regression_results_hips[method_name] = {\n",
    "                    'slope': slope,\n",
    "                    'intercept': intercept,\n",
    "                    'r_squared': r_squared,\n",
    "                    'data_points': mask.sum()\n",
    "                }\n",
    "    \n",
    "    # Format plots\n",
    "    for i, ax in enumerate(axes):\n",
    "        y_label = 'FTIR EC (Î¼g/mÂ³)' if i == 0 else 'HIPS Fabs'\n",
    "        ax.set_xlabel(f'Aethalometer {wavelength} BC (Î¼g/mÂ³)', fontsize=12)\n",
    "        ax.set_ylabel(y_label, fontsize=12)\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Make sure axes start at 0 if all data is positive\n",
    "        ax.set_xlim(left=0)\n",
    "        ax.set_ylim(bottom=0)\n",
    "    \n",
    "    # Add a summary table for each regression\n",
    "    if regression_results_ftir or regression_results_hips:\n",
    "        table_text = \"Regression Results Summary:\\n\\n\"\n",
    "        \n",
    "        # FTIR results\n",
    "        if regression_results_ftir:\n",
    "            table_text += \"FTIR EC Comparison:\\n\"\n",
    "            table_text += \"-\" * 60 + \"\\n\"\n",
    "            table_text += f\"{'Method':<10} {'Slope':<10} {'Intercept':<10} {'RÂ²':<10} {'N':<10}\\n\"\n",
    "            table_text += \"-\" * 60 + \"\\n\"\n",
    "            \n",
    "            for method, results in regression_results_ftir.items():\n",
    "                table_text += f\"{method:<10} {results['slope']:<10.3f} {results['intercept']:<10.3f} \"\n",
    "                table_text += f\"{results['r_squared']:<10.3f} {results['data_points']:<10}\\n\"\n",
    "        \n",
    "        # HIPS results\n",
    "        if regression_results_hips:\n",
    "            if regression_results_ftir:\n",
    "                table_text += \"\\n\"\n",
    "            table_text += \"HIPS Fabs Comparison:\\n\"\n",
    "            table_text += \"-\" * 60 + \"\\n\"\n",
    "            table_text += f\"{'Method':<10} {'Slope':<10} {'Intercept':<10} {'RÂ²':<10} {'N':<10}\\n\"\n",
    "            table_text += \"-\" * 60 + \"\\n\"\n",
    "            \n",
    "            for method, results in regression_results_hips.items():\n",
    "                table_text += f\"{method:<10} {results['slope']:<10.3f} {results['intercept']:<10.3f} \"\n",
    "                table_text += f\"{results['r_squared']:<10.3f} {results['data_points']:<10}\\n\"\n",
    "        \n",
    "        fig.text(0.5, 0.01, table_text, ha='center', va='bottom', \n",
    "                fontsize=10, family='monospace', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.25)  # Adjust for table\n",
    "    \n",
    "    return fig, {'ftir': regression_results_ftir, 'hips': regression_results_hips}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seasonal_comparison_smoothed(overlap_df, smoothing_method, wavelength=\"Red\"):\n",
    "    \"\"\"\n",
    "    Create seasonal comparison plots for a specific smoothing method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    overlap_df : pandas.DataFrame\n",
    "        DataFrame with overlapping periods including all smoothing methods\n",
    "    smoothing_method : str\n",
    "        Smoothing method to plot ('raw', 'ona', 'cma', 'dema')\n",
    "    wavelength : str\n",
    "        Wavelength used for BC data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        Figure with the plots\n",
    "    \"\"\"\n",
    "    # Column mapping based on smoothing method\n",
    "    method_column_map = {\n",
    "        'raw': 'aeth_mean',\n",
    "        'ona': 'ona_mean',\n",
    "        'cma': 'cma_mean',\n",
    "        'dema': 'dema_mean'\n",
    "    }\n",
    "    \n",
    "    method_display_names = {\n",
    "        'raw': 'Raw',\n",
    "        'ona': 'ONA',\n",
    "        'cma': 'CMA',\n",
    "        'dema': 'DEMA'\n",
    "    }\n",
    "    \n",
    "    # Get the correct column\n",
    "    bc_col = method_column_map.get(smoothing_method.lower())\n",
    "    if bc_col not in overlap_df.columns:\n",
    "        print(f\"Error: Column {bc_col} not found in overlap_df\")\n",
    "        return None\n",
    "    \n",
    "    # Add date and season information\n",
    "    df = overlap_df.copy()\n",
    "    if 'date' not in df.columns:\n",
    "        df['date'] = df['start_time'].dt.date\n",
    "    \n",
    "    if 'month' not in df.columns:\n",
    "        df['month'] = df['start_time'].dt.month\n",
    "    \n",
    "    # Map Ethiopian seasons\n",
    "    def map_ethiopian_seasons(month):\n",
    "        \"\"\"Maps month number to Ethiopian season name.\"\"\"\n",
    "        if month in [10, 11, 12, 1, 2]:\n",
    "            return 'Dry Season'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Belg Rainy Season'\n",
    "        else:  # months 6â€“9\n",
    "            return 'Kiremt Rainy Season'\n",
    "    \n",
    "    df['season'] = df['month'].apply(map_ethiopian_seasons)\n",
    "    \n",
    "    # Convert Aethalometer BC from ng/mÂ³ to Î¼g/mÂ³\n",
    "    df['bc_ug'] = df[bc_col] / 1000\n",
    "    \n",
    "    # Define colors for seasons\n",
    "    season_colors = {\n",
    "        'Dry Season': 'gold',\n",
    "        'Belg Rainy Season': 'limegreen',\n",
    "        'Kiremt Rainy Season': 'royalblue'\n",
    "    }\n",
    "    \n",
    "    # Create figure with 2 subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # Plot 1: BC vs EC_FTIR by season\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Separate scatter plots for each season\n",
    "    for season, color in season_colors.items():\n",
    "        season_data = df[df['season'] == season]\n",
    "        if len(season_data) > 0:\n",
    "            ax1.scatter(\n",
    "                season_data['bc_ug'], \n",
    "                season_data['EC_FTIR'],\n",
    "                alpha=0.7, \n",
    "                s=80, \n",
    "                c=color, \n",
    "                edgecolor='black',\n",
    "                label=f\"{season} (n={len(season_data)})\"\n",
    "            )\n",
    "    \n",
    "    # Overall trendline\n",
    "    mask1 = ~np.isnan(df['bc_ug']) & ~np.isnan(df['EC_FTIR'])\n",
    "    \n",
    "    if mask1.sum() > 1:\n",
    "        slope1, intercept1 = np.polyfit(df.loc[mask1, 'bc_ug'], df.loc[mask1, 'EC_FTIR'], 1)\n",
    "        x_line1 = np.linspace(df['bc_ug'].min(), df['bc_ug'].max(), 100)\n",
    "        y_line1 = slope1 * x_line1 + intercept1\n",
    "        ax1.plot(x_line1, y_line1, 'r--', linewidth=2, label='All Seasons Fit')\n",
    "        \n",
    "        # Add regression equation and RÂ²\n",
    "        r_value1 = pearsonr(df.loc[mask1, 'bc_ug'], df.loc[mask1, 'EC_FTIR'])[0]\n",
    "        r_squared1 = r_value1**2\n",
    "        \n",
    "        equation1 = f\"y = {slope1:.3f}x + {intercept1:.3f}\"\n",
    "        r2_text1 = f\"RÂ² = {r_squared1:.3f}\"\n",
    "        \n",
    "        ax1.annotate(\n",
    "            equation1 + '\\n' + r2_text1,\n",
    "            xy=(0.05, 0.95),\n",
    "            xycoords='axes fraction',\n",
    "            fontsize=12,\n",
    "            ha='left',\n",
    "            va='top',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8)\n",
    "        )\n",
    "    \n",
    "    # Format plot 1\n",
    "    ax1.set_xlabel(f'Aethalometer {wavelength} BC (Î¼g/mÂ³) - {method_display_names[smoothing_method.lower()]}', fontsize=12)\n",
    "    ax1.set_ylabel('FTIR EC (Î¼g/mÂ³)', fontsize=12)\n",
    "    ax1.set_title(f\"{wavelength} BC ({method_display_names[smoothing_method.lower()]}) vs FTIR EC by Ethiopian Season\\n({mask1.sum()} samples)\", fontsize=14)\n",
    "    \n",
    "    # Add 1:1 line\n",
    "    max_val1 = max(df['bc_ug'].max(), df['EC_FTIR'].max()) * 1.1\n",
    "    ax1.plot([0, max_val1], [0, max_val1], 'k--', alpha=0.3, linewidth=1, label='1:1 Line')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot 2: BC vs HIPS Fabs by season\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    # Separate scatter plots for each season\n",
    "    for season, color in season_colors.items():\n",
    "        season_data = df[df['season'] == season]\n",
    "        if len(season_data) > 0:\n",
    "            ax2.scatter(\n",
    "                season_data['bc_ug'], \n",
    "                season_data['Fabs'],\n",
    "                alpha=0.7, \n",
    "                s=80, \n",
    "                c=color, \n",
    "                edgecolor='black',\n",
    "                label=f\"{season} (n={len(season_data)})\"\n",
    "            )\n",
    "    \n",
    "    # Overall trendline\n",
    "    mask2 = ~np.isnan(df['bc_ug']) & ~np.isnan(df['Fabs'])\n",
    "    \n",
    "    if mask2.sum() > 1:\n",
    "        slope2, intercept2 = np.polyfit(df.loc[mask2, 'bc_ug'], df.loc[mask2, 'Fabs'], 1)\n",
    "        x_line2 = np.linspace(df['bc_ug'].min(), df['bc_ug'].max(), 100)\n",
    "        y_line2 = slope2 * x_line2 + intercept2\n",
    "        ax2.plot(x_line2, y_line2, 'r--', linewidth=2, label='All Seasons Fit')\n",
    "        \n",
    "        # Add regression equation and RÂ²\n",
    "        r_value2 = pearsonr(df.loc[mask2, 'bc_ug'], df.loc[mask2, 'Fabs'])[0]\n",
    "        r_squared2 = r_value2**2\n",
    "        \n",
    "        equation2 = f\"y = {slope2:.3f}x + {intercept2:.3f}\"\n",
    "        r2_text2 = f\"RÂ² = {r_squared2:.3f}\"\n",
    "        \n",
    "        ax2.annotate(\n",
    "            equation2 + '\\n' + r2_text2,\n",
    "            xy=(0.05, 0.95),\n",
    "            xycoords='axes fraction',\n",
    "            fontsize=12,\n",
    "            ha='left',\n",
    "            va='top',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8)\n",
    "        )\n",
    "    \n",
    "    # Format plot 2\n",
    "    ax2.set_xlabel(f'Aethalometer {wavelength} BC (Î¼g/mÂ³) - {method_display_names[smoothing_method.lower()]}', fontsize=12)\n",
    "    ax2.set_ylabel('HIPS Fabs', fontsize=12)\n",
    "    ax2.set_title(f\"{wavelength} BC ({method_display_names[smoothing_method.lower()]}) vs HIPS Fabs by Ethiopian Season\\n({mask2.sum()} samples)\", fontsize=14)\n",
    "    \n",
    "    # Make sure axes start at 0 if all data is positive\n",
    "    for ax, mask in [(ax1, mask1), (ax2, mask2)]:\n",
    "        if df.loc[mask, 'bc_ug'].min() >= 0:\n",
    "            ax.set_xlim(left=0)\n",
    "        if (ax == ax1 and df.loc[mask, 'EC_FTIR'].min() >= 0) or (ax == ax2 and df.loc[mask, 'Fabs'].min() >= 0):\n",
    "            ax.set_ylim(bottom=0)\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create seasonal regression stats table\n",
    "    season_stats = []\n",
    "    for season in season_colors.keys():\n",
    "        season_data = df[df['season'] == season]\n",
    "        if len(season_data) > 1:  # Need at least 2 points for regression\n",
    "            # For BC vs EC_FTIR\n",
    "            mask_ec = ~np.isnan(season_data['bc_ug']) & ~np.isnan(season_data['EC_FTIR'])\n",
    "            \n",
    "            if mask_ec.sum() > 1:\n",
    "                slope_ec, intercept_ec = np.polyfit(\n",
    "                    season_data.loc[mask_ec, 'bc_ug'], \n",
    "                    season_data.loc[mask_ec, 'EC_FTIR'], \n",
    "                    1\n",
    "                )\n",
    "                r_value_ec = pearsonr(\n",
    "                    season_data.loc[mask_ec, 'bc_ug'], \n",
    "                    season_data.loc[mask_ec, 'EC_FTIR']\n",
    "                )[0]\n",
    "                r_squared_ec = r_value_ec**2\n",
    "                \n",
    "                # For BC vs HIPS Fabs\n",
    "                mask_fabs = ~np.isnan(season_data['bc_ug']) & ~np.isnan(season_data['Fabs'])\n",
    "                \n",
    "                if mask_fabs.sum() > 1:\n",
    "                    slope_fabs, intercept_fabs = np.polyfit(\n",
    "                        season_data.loc[mask_fabs, 'bc_ug'],\n",
    "                        season_data.loc[mask_fabs, 'Fabs'],\n",
    "                        1\n",
    "                    )\n",
    "                    r_value_fabs = pearsonr(\n",
    "                        season_data.loc[mask_fabs, 'bc_ug'],\n",
    "                        season_data.loc[mask_fabs, 'Fabs']\n",
    "                    )[0]\n",
    "                    r_squared_fabs = r_value_fabs**2\n",
    "                    \n",
    "                    # Add to stats table\n",
    "                    season_stats.append({\n",
    "                        'Season': season,\n",
    "                        'Count': len(season_data),\n",
    "                        'EC_FTIR_Slope': slope_ec,\n",
    "                        'EC_FTIR_Intercept': intercept_ec,\n",
    "                        'EC_FTIR_R2': r_squared_ec,\n",
    "                        'Fabs_Slope': slope_fabs,\n",
    "                        'Fabs_Intercept': intercept_fabs,\n",
    "                        'Fabs_R2': r_squared_fabs,\n",
    "                        'Method': method_display_names[smoothing_method.lower()]\n",
    "                    })\n",
    "    \n",
    "    # Create and return seasonal stats DataFrame\n",
    "    if season_stats:\n",
    "        season_stats_df = pd.DataFrame(season_stats)\n",
    "        print(f\"\\nSeasonal Regression Statistics ({method_display_names[smoothing_method.lower()]}):\")\n",
    "        print(season_stats_df.round(3))\n",
    "        return fig, season_stats_df\n",
    "    \n",
    "    return fig, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c9f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mac_values_all_methods(overlap_df, wavelength=\"Red\"):\n",
    "    \"\"\"\n",
    "    Calculate Mass Absorption Cross-section (MAC) values for all smoothing methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    overlap_df : pandas.DataFrame\n",
    "        DataFrame with overlapping periods including all smoothing methods\n",
    "    wavelength : str\n",
    "        Wavelength used for BC data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    mac_df : pandas.DataFrame\n",
    "        DataFrame with MAC values for each method\n",
    "    fig : matplotlib.figure.Figure\n",
    "        Figure with MAC distributions\n",
    "    \"\"\"\n",
    "    # Method column mappings\n",
    "    method_columns = {\n",
    "        'Raw': 'aeth_mean',\n",
    "        'ONA': 'ona_mean',\n",
    "        'CMA': 'cma_mean',\n",
    "        'DEMA': 'dema_mean'\n",
    "    }\n",
    "    \n",
    "    # Create a copy of the data\n",
    "    mac_df = overlap_df.copy()\n",
    "    \n",
    "    # Add date and season information if not already present\n",
    "    if 'date' not in mac_df.columns and 'start_time' in mac_df.columns:\n",
    "        mac_df['date'] = mac_df['start_time'].dt.date\n",
    "    \n",
    "    if 'month' not in mac_df.columns and 'start_time' in mac_df.columns:\n",
    "        mac_df['month'] = mac_df['start_time'].dt.month\n",
    "    \n",
    "    if 'season' not in mac_df.columns and 'month' in mac_df.columns:\n",
    "        # Map Ethiopian seasons\n",
    "        def map_ethiopian_seasons(month):\n",
    "            \"\"\"Maps month number to Ethiopian season name.\"\"\"\n",
    "            if month in [10, 11, 12, 1, 2]:\n",
    "                return 'Dry Season'\n",
    "            elif month in [3, 4, 5]:\n",
    "                return 'Belg Rainy Season'\n",
    "            else:  # months 6â€“9\n",
    "                return 'Kiremt Rainy Season'\n",
    "        \n",
    "        mac_df['season'] = mac_df['month'].apply(map_ethiopian_seasons)\n",
    "    \n",
    "    # Calculate MAC values for each method\n",
    "    mac_results = {}\n",
    "    \n",
    "    for method_name, bc_col in method_columns.items():\n",
    "        if bc_col in mac_df.columns:\n",
    "            # Convert BC from ng/mÂ³ to Î¼g/mÂ³\n",
    "            mac_df[f'{method_name}_BC_ug'] = mac_df[bc_col] / 1000\n",
    "            \n",
    "            # Calculate MAC (mÂ²/g) = Fabs / EC_FTIR\n",
    "            mac_df[f'{method_name}_MAC'] = mac_df['Fabs'] / mac_df['EC_FTIR']\n",
    "            \n",
    "            # Filter out invalid MAC values\n",
    "            valid_mac = mac_df[(mac_df[f'{method_name}_MAC'] > 0) & (mac_df[f'{method_name}_MAC'] < 30)]\n",
    "            \n",
    "            if len(valid_mac) > 0:\n",
    "                # Calculate statistics\n",
    "                overall_mac = valid_mac[f'{method_name}_MAC'].mean()\n",
    "                seasonal_macs = valid_mac.groupby('season')[f'{method_name}_MAC'].agg(['mean', 'std', 'count'])\n",
    "                \n",
    "                # Store results\n",
    "                mac_results[method_name] = {\n",
    "                    'overall_mac': overall_mac,\n",
    "                    'seasonal_macs': seasonal_macs,\n",
    "                    'valid_count': len(valid_mac)\n",
    "                }\n",
    "    \n",
    "    # Print summary of MAC values\n",
    "    print(\"\\nMass Absorption Cross-section (MAC) Values Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for method_name, results in mac_results.items():\n",
    "        print(f\"\\n{method_name} Processing:\")\n",
    "        print(f\"Overall MAC: {results['overall_mac']:.2f} mÂ²/g (n={results['valid_count']})\")\n",
    "        print(\"\\nSeasonal MAC Values:\")\n",
    "        print(results['seasonal_macs'].round(2))\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot MAC distributions for each method\n",
    "    for i, (method_name, results) in enumerate(mac_results.items()):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # MAC histogram\n",
    "            valid_mac = mac_df[(mac_df[f'{method_name}_MAC'] > 0) & (mac_df[f'{method_name}_MAC'] < 30)]\n",
    "            ax.hist(valid_mac[f'{method_name}_MAC'], bins=15, alpha=0.7, color='crimson', edgecolor='black')\n",
    "            ax.axvline(results['overall_mac'], color='black', linestyle='--', linewidth=2, \n",
    "                      label=f'Mean MAC: {results[\"overall_mac\"]:.2f} mÂ²/g')\n",
    "            \n",
    "            ax.set_xlabel('MAC (mÂ²/g)', fontsize=12)\n",
    "            ax.set_ylabel('Frequency', fontsize=12)\n",
    "            ax.set_title(f'{method_name} Processing - MAC Distribution\\n(n={results[\"valid_count\"]})', fontsize=14)\n",
    "            ax.legend()\n",
    "            ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Hide empty subplots if fewer than 4 methods\n",
    "    for i in range(len(mac_results), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create a comparison table of MAC values\n",
    "    mac_comparison = pd.DataFrame({\n",
    "        'Method': [],\n",
    "        'Overall MAC': [],\n",
    "        'Sample Count': []\n",
    "    })\n",
    "    \n",
    "    for method_name, results in mac_results.items():\n",
    "        mac_comparison = pd.concat([mac_comparison, pd.DataFrame({\n",
    "            'Method': [method_name],\n",
    "            'Overall MAC': [results['overall_mac']],\n",
    "            'Sample Count': [results['valid_count']]\n",
    "        })], ignore_index=True)\n",
    "    \n",
    "    # Add seasonal MAC values to the comparison\n",
    "    for season in ['Dry Season', 'Belg Rainy Season', 'Kiremt Rainy Season']:\n",
    "        mac_comparison[f'{season} MAC'] = np.nan\n",
    "        \n",
    "        for i, (method_name, results) in enumerate(mac_results.items()):\n",
    "            seasonal_data = results['seasonal_macs']\n",
    "            if season in seasonal_data.index:\n",
    "                mac_comparison.loc[i, f'{season} MAC'] = seasonal_data.loc[season, 'mean']\n",
    "    \n",
    "    print(\"\\nMAC Comparison Across Methods:\")\n",
    "    print(mac_comparison.round(2))\n",
    "    \n",
    "    return mac_df, fig, mac_comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8b37b8",
   "metadata": {},
   "source": [
    "## Step 7: Main Execution Function\n",
    " \n",
    "This function brings together all the components to perform the complete analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to perform the black carbon cross-method analysis\n",
    "    with multiple smoothing techniques.\n",
    "    \"\"\"\n",
    "    print(\"Black Carbon Cross-Method Analysis with Smoothing Techniques\")\n",
    "    print(\"==========================================================\")\n",
    "    print(\"This script analyzes excellent quality data from:\")\n",
    "    print(\"1. Aethalometer (Red BCc) - with Raw, ONA, CMA, and DEMA processing\")\n",
    "    print(\"2. FTIR (Elemental Carbon)\")\n",
    "    print(\"3. HIPS (Filter light absorption)\")\n",
    "    print(\"==========================================================\\n\")\n",
    "    \n",
    "    # Define file paths - adjust these to your actual file paths\n",
    "    aethalometer_path = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data/Aethelometry Data/Jacros_MA350_1-min_2022-2024_Cleaned.csv\"\n",
    "    db_path = \"/Users/ahzs645/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data/EC-HIPS-Aeth Comparison/Data/Original Data/Combined Database/spartan_ftir_hips.db\"\n",
    "    quality_file_path = \"all_filter_dates_with_complete_quality_9to9.csv\"\n",
    "    \n",
    "    # Load Aethalometer data\n",
    "    print(\"\\n--- Loading Aethalometer Data ---\")\n",
    "    aethalometer_df = load_aethalometer_data(aethalometer_path)\n",
    "    \n",
    "    if aethalometer_df is None:\n",
    "        print(\"ERROR: Failed to load Aethalometer data. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Load filter sample data\n",
    "    print(\"\\n--- Loading Filter Sample Data ---\")\n",
    "    etad_data, ftir_data = load_filter_sample_data(db_path)\n",
    "    \n",
    "    if len(etad_data) == 0:\n",
    "        print(\"ERROR: No filter sample data available. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Load or identify excellent periods\n",
    "    print(\"\\n--- Identifying Excellent Quality Periods ---\")\n",
    "    quality_df = load_quality_periods(quality_file_path)\n",
    "    excellent_periods = identify_excellent_periods(aethalometer_df, quality_df)\n",
    "    \n",
    "    if excellent_periods is None or len(excellent_periods) == 0:\n",
    "        print(\"ERROR: No excellent quality periods identified. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Apply all smoothing methods to the Aethalometer data\n",
    "    print(\"\\n--- Applying Smoothing Methods ---\")\n",
    "    smoothed_data = apply_all_smoothing_methods(aethalometer_df, wavelength=\"Red\")\n",
    "    \n",
    "    # Find overlapping periods with filter samples (using raw data)\n",
    "    print(\"\\n--- Finding Overlapping Periods ---\")\n",
    "    overlap_df_raw = find_overlapping_excellent_data(aethalometer_df, etad_data, excellent_periods)\n",
    "    \n",
    "    if overlap_df_raw is None or len(overlap_df_raw) == 0:\n",
    "        print(\"ERROR: No overlapping periods found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Add smoothed data to the overlap DataFrame\n",
    "    print(\"\\n--- Adding Smoothed Data to Overlap DataFrame ---\")\n",
    "    overlap_df_all = create_overlap_df_with_all_methods(overlap_df_raw, smoothed_data, wavelength=\"Red\")\n",
    "    \n",
    "    # Create visualization plots\n",
    "    print(\"\\n--- Creating Cross-Method Plots with All Smoothing Methods ---\")\n",
    "    print(f\"Using {len(overlap_df_all)} days with excellent data from all methods\")\n",
    "    \n",
    "    # Compare all methods with FTIR EC\n",
    "    fig_ftir, reg_results_ftir = plot_bc_scatter_all_methods(\n",
    "        overlap_df_all, y_param='EC_FTIR', wavelength=\"Red\"\n",
    "    )\n",
    "    plt.savefig(\"comparison_all_methods_with_ftir.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig_ftir)\n",
    "    \n",
    "    # Compare all methods with HIPS Fabs\n",
    "    fig_hips, reg_results_hips = plot_bc_scatter_all_methods(\n",
    "        overlap_df_all, y_param='Fabs', wavelength=\"Red\"\n",
    "    )\n",
    "    plt.savefig(\"comparison_all_methods_with_hips.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig_hips)\n",
    "    \n",
    "    # Method comparison plot (combined)\n",
    "    fig_methods, reg_results_combined = plot_method_comparison_all_smoothing(\n",
    "        overlap_df_all, wavelength=\"Red\"\n",
    "    )\n",
    "    plt.savefig(\"method_comparison_combined.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig_methods)\n",
    "    \n",
    "    # Seasonal comparisons for each method\n",
    "    for method in ['raw', 'ona', 'cma', 'dema']:\n",
    "        fig_seasonal, seasonal_stats = plot_seasonal_comparison_smoothed(\n",
    "            overlap_df_all, method, wavelength=\"Red\"\n",
    "        )\n",
    "        plt.savefig(f\"seasonal_comparison_{method}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig_seasonal)\n",
    "    \n",
    "    # Calculate MAC values for all methods\n",
    "    mac_df, fig_mac, mac_comparison = calculate_mac_values_all_methods(\n",
    "        overlap_df_all, wavelength=\"Red\"\n",
    "    )\n",
    "    plt.savefig(\"mac_values_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig_mac)\n",
    "    \n",
    "    # Save the processed data and results\n",
    "    print(\"\\n--- Saving Results ---\")\n",
    "    overlap_df_all.to_csv(\"overlap_data_with_all_smoothing_methods.csv\", index=True)\n",
    "    mac_comparison.to_csv(\"mac_comparison_results.csv\", index=False)\n",
    "    \n",
    "    # Create a summary of regression results\n",
    "    regression_summary = {\n",
    "        'FTIR_EC': reg_results_ftir,\n",
    "        'HIPS_Fabs': reg_results_hips,\n",
    "        'Combined': reg_results_combined\n",
    "    }\n",
    "    \n",
    "    print(\"\\n--- Analysis Complete ---\")\n",
    "    print(f\"Successfully analyzed {len(overlap_df_all)} overlapping excellent quality days with multiple smoothing methods\")\n",
    "    \n",
    "    # Return results for further analysis if needed\n",
    "    return {\n",
    "        'aethalometer_df': aethalometer_df,\n",
    "        'smoothed_data': smoothed_data,\n",
    "        'filter_df': etad_data,\n",
    "        'excellent_periods': excellent_periods,\n",
    "        'overlap_df_all': overlap_df_all,\n",
    "        'mac_df': mac_df,\n",
    "        'mac_comparison': mac_comparison,\n",
    "        'regression_summary': regression_summary\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad8252",
   "metadata": {},
   "source": [
    "## Run the Analysis\n",
    " \n",
    "Execute the main function to perform the complete black carbon cross-method analysis with multiple smoothing methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caddccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23100a",
   "metadata": {},
   "source": [
    "## Explore Results\n",
    " \n",
    "After running the analysis, you can explore the results further. Here are some examples of additional analyses:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed608e12",
   "metadata": {},
   "source": [
    "Explore the regression results in detail\n",
    "Uncomment and run after executing the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc8d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'regression_summary' in results:\n",
    "    reg_summary = results['regression_summary']\n",
    "    \n",
    "    print(\"\\nReduced Ordinary Least Squares (OLS) Equations:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # FTIR EC comparison\n",
    "    if 'FTIR_EC' in reg_summary:\n",
    "        print(\"\\nFTIR EC vs BC:\")\n",
    "        for method, stats in reg_summary['FTIR_EC'].items():\n",
    "            print(f\"{method}: EC = {stats['slope']:.3f} Ã— BC + {stats['intercept']:.3f}  (RÂ² = {stats['r_squared']:.3f}, n = {stats['data_points']})\")\n",
    "    \n",
    "    # HIPS Fabs comparison\n",
    "    if 'HIPS_Fabs' in reg_summary:\n",
    "        print(\"\\nHIPS Fabs vs BC:\")\n",
    "        for method, stats in reg_summary['HIPS_Fabs'].items():\n",
    "            print(f\"{method}: Fabs = {stats['slope']:.3f} Ã— BC + {stats['intercept']:.3f}  (RÂ² = {stats['r_squared']:.3f}, n = {stats['data_points']})\")\n",
    "    \n",
    "    # MAC comparison\n",
    "    if 'mac_comparison' in results:\n",
    "        mac_comp = results['mac_comparison']\n",
    "        \n",
    "        print(\"\\nMAC Values by Method and Season:\")\n",
    "        print(\"=\" * 70)\n",
    "        print(mac_comp.round(2).to_string(index=False))\n",
    "        \n",
    "        # Calculate method-to-method ratios for MAC values\n",
    "        if len(mac_comp) > 1:\n",
    "            print(\"\\nMAC Ratios (Row Method / Column Method):\")\n",
    "            methods = mac_comp['Method'].tolist()\n",
    "            ratio_matrix = np.zeros((len(methods), len(methods)))\n",
    "            \n",
    "            for i, method1 in enumerate(methods):\n",
    "                for j, method2 in enumerate(methods):\n",
    "                    if i != j:\n",
    "                        mac1 = mac_comp.loc[mac_comp['Method'] == method1, 'Overall MAC'].values[0]\n",
    "                        mac2 = mac_comp.loc[mac_comp['Method'] == method2, 'Overall MAC'].values[0]\n",
    "                        ratio_matrix[i, j] = mac1 / mac2\n",
    "            \n",
    "            ratio_df = pd.DataFrame(ratio_matrix, index=methods, columns=methods)\n",
    "            print(ratio_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d63b57e",
   "metadata": {},
   "source": [
    "Analyze the performance improvements by smoothing method\n",
    "Uncomment and run after executing the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'overlap_df_all' in results:\n",
    "    overlap_df = results['overlap_df_all']\n",
    "    \n",
    "    # Calculate improvement metrics\n",
    "    methods = ['ona', 'cma', 'dema']\n",
    "    improvement_metrics = []\n",
    "    \n",
    "    for method in methods:\n",
    "        # Get column names\n",
    "        raw_col = 'aeth_mean'\n",
    "        method_col = f'{method}_mean'\n",
    "        \n",
    "        if raw_col in overlap_df.columns and method_col in overlap_df.columns:\n",
    "            # Calculate metrics vs FTIR\n",
    "            if 'EC_FTIR' in overlap_df.columns:\n",
    "                # Convert to Î¼g/mÂ³\n",
    "                raw_vs_ftir = pearsonr(overlap_df[raw_col] / 1000, overlap_df['EC_FTIR'])[0]**2\n",
    "                method_vs_ftir = pearsonr(overlap_df[method_col] / 1000, overlap_df['EC_FTIR'])[0]**2\n",
    "                ftir_improvement = (method_vs_ftir - raw_vs_ftir) / raw_vs_ftir * 100\n",
    "            else:\n",
    "                raw_vs_ftir = np.nan\n",
    "                method_vs_ftir = np.nan\n",
    "                ftir_improvement = np.nan\n",
    "            \n",
    "            # Calculate metrics vs HIPS\n",
    "            if 'Fabs' in overlap_df.columns:\n",
    "                raw_vs_hips = pearsonr(overlap_df[raw_col] / 1000, overlap_df['Fabs'])[0]**2\n",
    "                method_vs_hips = pearsonr(overlap_df[method_col] / 1000, overlap_df['Fabs'])[0]**2\n",
    "                hips_improvement = (method_vs_hips - raw_vs_hips) / raw_vs_hips * 100\n",
    "            else:\n",
    "                raw_vs_hips = np.nan\n",
    "                method_vs_hips = np.nan\n",
    "                hips_improvement = np.nan\n",
    "            \n",
    "            # Calculate negative value reduction\n",
    "            raw_neg_pct = (overlap_df[raw_col] < 0).mean() * 100\n",
    "            method_neg_pct = (overlap_df[method_col] < 0).mean() * 100\n",
    "            neg_reduction = (raw_neg_pct - method_neg_pct) / raw_neg_pct * 100 if raw_neg_pct > 0 else 0\n",
    "            \n",
    "            # Add to metrics\n",
    "            improvement_metrics.append({\n",
    "                'Method': method.upper(),\n",
    "                'Raw RÂ² vs FTIR': raw_vs_ftir,\n",
    "                'Method RÂ² vs FTIR': method_vs_ftir,\n",
    "                'FTIR RÂ² Improvement (%)': ftir_improvement,\n",
    "                'Raw RÂ² vs HIPS': raw_vs_hips,\n",
    "                'Method RÂ² vs HIPS': method_vs_hips,\n",
    "                'HIPS RÂ² Improvement (%)': hips_improvement,\n",
    "                'Raw Negative Values (%)': raw_neg_pct,\n",
    "                'Method Negative Values (%)': method_neg_pct,\n",
    "                'Negative Value Reduction (%)': neg_reduction\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame and display\n",
    "    if improvement_metrics:\n",
    "        improvement_df = pd.DataFrame(improvement_metrics)\n",
    "        print(\"\\nImprovement Metrics by Smoothing Method:\")\n",
    "        print(\"=\" * 100)\n",
    "        print(improvement_df.round(2).to_string(index=False))\n",
    "        \n",
    "        # Create bar plot of improvements\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        methods = improvement_df['Method']\n",
    "        ftir_improvement = improvement_df['FTIR RÂ² Improvement (%)']\n",
    "        hips_improvement = improvement_df['HIPS RÂ² Improvement (%)']\n",
    "        neg_reduction = improvement_df['Negative Value Reduction (%)']\n",
    "        \n",
    "        # Set up bar positions\n",
    "        x = np.arange(len(methods))\n",
    "        width = 0.25\n",
    "        \n",
    "        # Create bars\n",
    "        plt.bar(x - width, ftir_improvement, width, label='FTIR RÂ² Improvement (%)', color='blue')\n",
    "        plt.bar(x, hips_improvement, width, label='HIPS RÂ² Improvement (%)', color='green')\n",
    "        plt.bar(x + width, neg_reduction, width, label='Negative Value Reduction (%)', color='red')\n",
    "        \n",
    "        # Add labels and formatting\n",
    "        plt.xlabel('Smoothing Method')\n",
    "        plt.ylabel('Percent Improvement')\n",
    "        plt.title('Performance Improvements by Smoothing Method')\n",
    "        plt.xticks(x, methods)\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"improvement_metrics_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
