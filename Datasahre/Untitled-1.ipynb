{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define global constants for BC columns and wavelengths\n",
    "BC_COLUMNS = [\n",
    "    'UV BC1', 'UV BC2', 'UV BCc',\n",
    "    'Blue BC1', 'Blue BC2', 'Blue BCc',\n",
    "    'Green BC1', 'Green BC2', 'Green BCc',\n",
    "    'Red BC1', 'Red BC2', 'Red BCc',\n",
    "    'IR BC1', 'IR BC2', 'IR BCc'\n",
    "]\n",
    "WAVELENGTHS = {'UV': 375, 'Blue': 470, 'Green': 528, 'Red': 625, 'IR': 880}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting problematic data examples...\n",
      "Loading raw data...\n",
      "\n",
      "Checking for non-numeric values in data columns...\n",
      "\n",
      "Processing data...\n",
      "\n",
      "Negative values removed:\n",
      "UV BC1: 1116 values\n",
      "UV BC2: 2034 values\n",
      "UV BCc: 26701 values\n",
      "Blue BC1: 791 values\n",
      "Blue BC2: 657 values\n",
      "Blue BCc: 18394 values\n",
      "Green BC1: 1318 values\n",
      "Green BC2: 1072 values\n",
      "Green BCc: 19824 values\n",
      "Red BC1: 919 values\n",
      "Red BC2: 721 values\n",
      "Red BCc: 21969 values\n",
      "IR BC1: 1104 values\n",
      "IR BC2: 597 values\n",
      "IR BCc: 30983 values\n",
      "\n",
      "Outliers removed:\n",
      "UV BC1: 22312 values\n",
      "UV BC2: 22918 values\n",
      "UV BCc: 22942 values\n",
      "Blue BC1: 22512 values\n",
      "Blue BC2: 23093 values\n",
      "Blue BCc: 140 values\n",
      "Green BC1: 22646 values\n",
      "Green BC2: 23171 values\n",
      "Green BCc: 16717 values\n",
      "Red BC1: 22804 values\n",
      "Red BC2: 23329 values\n",
      "Red BCc: 21484 values\n",
      "IR BC1: 23150 values\n",
      "IR BC2: 23615 values\n",
      "IR BCc: 22771 values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/4q55gl35679357bs417130z40000gn/T/ipykernel_57980/4167265038.py:71: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hourly_df = processed_df.resample('H').mean(numeric_only=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing missing hours in the resampled data...\n",
      "Missing hourly values (out of 20665 total hours):\n",
      "UV BC1       2367\n",
      "UV BC2       2367\n",
      "UV BCc       2366\n",
      "Blue BC1     2363\n",
      "Blue BC2     2362\n",
      "Blue BCc     2340\n",
      "Green BC1    2363\n",
      "Green BC2    2366\n",
      "Green BCc    2352\n",
      "Red BC1      2365\n",
      "Red BC2      2368\n",
      "Red BCc      2352\n",
      "IR BC1       2367\n",
      "IR BC2       2368\n",
      "IR BCc       2345\n",
      "dtype: int64\n",
      "\n",
      "Completely missing hours (all columns): 2339\n",
      "\n",
      "Longest periods of completely missing data:\n",
      "1. From 2022-06-23 23:00:00+03:00 to 2022-07-27 10:00:00+03:00 (804 hours)\n",
      "2. From 2023-06-21 07:00:00+03:00 to 2023-06-29 09:00:00+03:00 (195 hours)\n",
      "3. From 2023-12-06 08:00:00+03:00 to 2023-12-12 09:00:00+03:00 (146 hours)\n",
      "4. From 2023-01-17 21:00:00+03:00 to 2023-01-23 04:00:00+03:00 (128 hours)\n",
      "5. From 2024-04-04 01:00:00+03:00 to 2024-04-08 20:00:00+03:00 (116 hours)\n",
      "6. From 2024-07-25 19:00:00+03:00 to 2024-07-30 10:00:00+03:00 (112 hours)\n",
      "7. From 2023-02-18 08:00:00+03:00 to 2023-02-22 06:00:00+03:00 (95 hours)\n",
      "8. From 2023-02-05 19:00:00+03:00 to 2023-02-09 05:00:00+03:00 (83 hours)\n",
      "9. From 2023-04-13 06:00:00+03:00 to 2023-04-15 09:00:00+03:00 (52 hours)\n",
      "10. From 2023-02-02 07:00:00+03:00 to 2023-02-03 20:00:00+03:00 (38 hours)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/4q55gl35679357bs417130z40000gn/T/ipykernel_57980/4167265038.py:77: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data visualizations saved as separate files.\n",
      "\n",
      "Exploring alternative resampling methods...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/4q55gl35679357bs417130z40000gn/T/ipykernel_57980/4167265038.py:163: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  two_hour_df = raw_df_copy[bc_columns].resample('2H').mean()\n",
      "/var/folders/q8/4q55gl35679357bs417130z40000gn/T/ipykernel_57980/4167265038.py:167: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  thirty_min_df = raw_df_copy[bc_columns].resample('30T').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data completeness comparison:\n",
      "              Daily    2-Hour  30-Minute\n",
      "UV BC1     0.922274  0.888416   0.885864\n",
      "UV BC2     0.922274  0.888416   0.885864\n",
      "UV BCc     0.922274  0.888416   0.885864\n",
      "Blue BC1   0.922274  0.888416   0.885864\n",
      "Blue BC2   0.922274  0.888416   0.885864\n",
      "Blue BCc   0.922274  0.888416   0.885864\n",
      "Green BC1  0.922274  0.888416   0.885864\n",
      "Green BC2  0.922274  0.888416   0.885864\n",
      "Green BCc  0.922274  0.888416   0.885864\n",
      "Red BC1    0.922274  0.888416   0.885864\n",
      "Red BC2    0.922274  0.888416   0.885864\n",
      "Red BCc    0.922274  0.888416   0.885864\n",
      "IR BC1     0.922274  0.888416   0.885864\n",
      "IR BC2     0.922274  0.888416   0.885864\n",
      "IR BCc     0.922274  0.888416   0.885864\n",
      "\n",
      "Calculating diurnal variations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/4q55gl35679357bs417130z40000gn/T/ipykernel_57980/4167265038.py:201: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hourly_df = df_copy[bc_columns].resample('H').mean(numeric_only=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full analysis complete. Check generated visualizations and 'missing_data_analysis_report.txt'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_missing_data(filepath):\n",
    "    # Part 1: Load the raw data\n",
    "    print(\"Loading raw data...\")\n",
    "    raw_df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Convert 'Time (UTC)' to datetime and then create local time\n",
    "    raw_df['Time (UTC)'] = pd.to_datetime(raw_df['Time (UTC)'], utc=True)\n",
    "    raw_df['Time (Local)'] = raw_df['Time (UTC)'].dt.tz_convert('Africa/Addis_Ababa')\n",
    "    \n",
    "    # Check for non-numeric values in BC columns and force conversion\n",
    "    print(\"\\nChecking for non-numeric values in data columns...\")\n",
    "    non_numeric_data = {}\n",
    "    for col in BC_COLUMNS:\n",
    "        if col in raw_df.columns:\n",
    "            converted = pd.to_numeric(raw_df[col], errors='coerce')\n",
    "            non_numeric_mask = converted.isna() & raw_df[col].notna()\n",
    "            non_numeric_count = non_numeric_mask.sum()\n",
    "            \n",
    "            if non_numeric_count > 0:\n",
    "                print(f\"WARNING: Found {non_numeric_count} non-numeric values in {col}\")\n",
    "                non_numeric_data[col] = raw_df.loc[non_numeric_mask, col].value_counts().to_dict()\n",
    "                print(f\"Example non-numeric values: {list(non_numeric_data[col].keys())[:3]}\")\n",
    "                \n",
    "            # Replace the column with its numeric conversion\n",
    "            raw_df[col] = converted\n",
    "    \n",
    "    # Part 2: Data processing steps\n",
    "    print(\"\\nProcessing data...\")\n",
    "    processed_df = raw_df.copy()\n",
    "    \n",
    "    removed_data = {\n",
    "        'non_numeric_values': non_numeric_data,\n",
    "        'negative_values': {},\n",
    "        'outliers': {}\n",
    "    }\n",
    "    \n",
    "    # Convert from ng/m³ to µg/m³ for BC columns\n",
    "    for col in BC_COLUMNS:\n",
    "        if col in processed_df.columns:\n",
    "            processed_df[col] = processed_df[col] / 1000\n",
    "        else:\n",
    "            print(f\"Warning: Column {col} not found - skipping\")\n",
    "    \n",
    "    # Remove negative values\n",
    "    for col in BC_COLUMNS:\n",
    "        if col in processed_df.columns:\n",
    "            neg_mask = processed_df[col] < 0\n",
    "            removed_data['negative_values'][col] = neg_mask.sum()\n",
    "            processed_df.loc[neg_mask, col] = np.nan\n",
    "    \n",
    "    # Remove outliers (values greater than mean + 3*std)\n",
    "    for col in BC_COLUMNS:\n",
    "        if col in processed_df.columns:\n",
    "            mean = processed_df[col].mean()\n",
    "            std = processed_df[col].std()\n",
    "            upper_limit = mean + 3 * std\n",
    "            outlier_mask = processed_df[col] > upper_limit\n",
    "            removed_data['outliers'][col] = outlier_mask.sum()\n",
    "            processed_df.loc[outlier_mask, col] = np.nan\n",
    "            \n",
    "    print(\"\\nNegative values removed:\")\n",
    "    for col, count in removed_data['negative_values'].items():\n",
    "        print(f\"{col}: {count} values\")\n",
    "    \n",
    "    print(\"\\nOutliers removed:\")\n",
    "    for col, count in removed_data['outliers'].items():\n",
    "        print(f\"{col}: {count} values\")\n",
    "    \n",
    "    # Part 3: Resample data to hourly averages\n",
    "    processed_df.set_index('Time (Local)', inplace=True)\n",
    "    hourly_df = processed_df.resample('H').mean(numeric_only=True)\n",
    "    \n",
    "    # Analyze missing hours in the resampled data\n",
    "    print(\"\\nAnalyzing missing hours in the resampled data...\")\n",
    "    start_date = hourly_df.index.min()\n",
    "    end_date = hourly_df.index.max()\n",
    "    full_date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "    full_df = pd.DataFrame(index=full_date_range)\n",
    "    merged_df = full_df.join(hourly_df)\n",
    "    \n",
    "    missing_hours = merged_df[BC_COLUMNS].isnull().sum()\n",
    "    print(f\"Missing hourly values (out of {len(full_date_range)} total hours):\")\n",
    "    print(missing_hours)\n",
    "    \n",
    "    completely_missing = merged_df[BC_COLUMNS].isnull().all(axis=1)\n",
    "    print(f\"\\nCompletely missing hours (all columns): {completely_missing.sum()}\")\n",
    "    \n",
    "    # Identify consecutive missing periods\n",
    "    missing_periods = []\n",
    "    current_period = None\n",
    "    for timestamp, is_missing in completely_missing.items():\n",
    "        if is_missing:\n",
    "            if current_period is None:\n",
    "                current_period = {\"start\": timestamp, \"end\": timestamp, \"count\": 1}\n",
    "            else:\n",
    "                current_period[\"end\"] = timestamp\n",
    "                current_period[\"count\"] += 1\n",
    "        else:\n",
    "            if current_period is not None:\n",
    "                missing_periods.append(current_period)\n",
    "                current_period = None\n",
    "    if current_period is not None:\n",
    "        missing_periods.append(current_period)\n",
    "    \n",
    "    missing_periods.sort(key=lambda x: x[\"count\"], reverse=True)\n",
    "    print(\"\\nLongest periods of completely missing data:\")\n",
    "    for i, period in enumerate(missing_periods[:10]):\n",
    "        print(f\"{i+1}. From {period['start']} to {period['end']} ({period['count']} hours)\")\n",
    "    \n",
    "    return raw_df, processed_df, hourly_df, merged_df, missing_periods, removed_data\n",
    "\n",
    "def visualize_missing_data(merged_df, bc_columns, missing_periods, removed_data):\n",
    "    # Plot 1: Daily missing data heatmap (separate figure)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_data = merged_df[bc_columns].isnull()\n",
    "    daily_missing = missing_data.resample('D').mean()\n",
    "    sns.heatmap(daily_missing.T, cmap='viridis', cbar_kws={'label': 'Fraction missing'})\n",
    "    plt.title('Missing Data Heatmap (Daily)')\n",
    "    plt.ylabel('BC Columns')\n",
    "    plt.xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('missing_data_heatmap_daily.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Bar chart of missing values by column\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_counts = merged_df[bc_columns].isnull().sum().sort_values(ascending=False)\n",
    "    missing_counts.plot(kind='bar')\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.ylabel('Missing Count')\n",
    "    plt.xlabel('BC Column')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('missing_values_by_column.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 3: Missing data by hour of day\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    hourly_missing = merged_df[bc_columns].isnull().groupby(merged_df.index.hour).mean()\n",
    "    sns.heatmap(hourly_missing.T, cmap='viridis', cbar_kws={'label': 'Fraction missing'})\n",
    "    plt.title('Missing Data by Hour of Day')\n",
    "    plt.ylabel('BC Columns')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('missing_data_by_hour.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Missing data visualizations saved as separate files.\")\n",
    "\n",
    "def alternative_resampling_methods(raw_df, bc_columns):\n",
    "    print(\"\\nExploring alternative resampling methods...\")\n",
    "    raw_df_copy = raw_df.copy()\n",
    "    raw_df_copy.set_index('Time (Local)', inplace=True)\n",
    "    \n",
    "    # Ensure BC columns are numeric\n",
    "    for col in bc_columns:\n",
    "        if col in raw_df_copy.columns:\n",
    "            raw_df_copy[col] = pd.to_numeric(raw_df_copy[col], errors='coerce')\n",
    "    \n",
    "    daily_df = raw_df_copy[bc_columns].resample('D').mean()\n",
    "    daily_completeness = 1 - daily_df.isnull().mean()\n",
    "    \n",
    "    two_hour_df = raw_df_copy[bc_columns].resample('2H').mean()\n",
    "    two_hour_completeness = 1 - two_hour_df.isnull().mean()\n",
    "    \n",
    "    # FIXED: use bc_columns (not bc_COLUMNS) to avoid NameError\n",
    "    thirty_min_df = raw_df_copy[bc_columns].resample('30T').mean()\n",
    "    thirty_min_completeness = 1 - thirty_min_df.isnull().mean()\n",
    "    \n",
    "    methods_comparison = pd.DataFrame({\n",
    "        'Daily': daily_completeness,\n",
    "        '2-Hour': two_hour_completeness,\n",
    "        '30-Minute': thirty_min_completeness\n",
    "    })\n",
    "    \n",
    "    print(\"\\nData completeness comparison:\")\n",
    "    print(methods_comparison)\n",
    "    \n",
    "    # Plot the completeness comparison in a separate figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    methods_comparison.plot(kind='bar')\n",
    "    plt.title('Data Completeness by Resampling Method')\n",
    "    plt.ylabel('Completeness (1.0 = complete)')\n",
    "    plt.xlabel('BC Column')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('resampling_methods_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return daily_df, two_hour_df, thirty_min_df, methods_comparison\n",
    "\n",
    "def calculate_diurnal_variations(raw_df, bc_columns, wavelengths):\n",
    "    print(\"\\nCalculating diurnal variations...\")\n",
    "    df_copy = raw_df.copy()\n",
    "    df_copy.set_index('Time (Local)', inplace=True)\n",
    "    \n",
    "    for col in bc_columns:\n",
    "        if col in df_copy.columns:\n",
    "            df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
    "    \n",
    "    hourly_df = df_copy[bc_columns].resample('H').mean(numeric_only=True)\n",
    "    hourly_df['Hour'] = hourly_df.index.hour\n",
    "    diurnal_pattern = hourly_df.groupby('Hour')[bc_columns].mean()\n",
    "    \n",
    "    df_copy['Hour'] = df_copy.index.hour\n",
    "    minute_diurnal = df_copy.groupby('Hour')[bc_columns].mean()\n",
    "    \n",
    "    # Plot 1: Diurnal variation from minute-level data (separate figure)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    selected_columns = ['UV BCc', 'Blue BCc', 'Green BCc', 'Red BCc', 'IR BCc']\n",
    "    for col in selected_columns:\n",
    "        if col in minute_diurnal.columns:\n",
    "            plt.plot(minute_diurnal.index, minute_diurnal[col], label=f\"{col.split()[0]} ({wavelengths[col.split()[0]]} nm)\")\n",
    "    plt.title('Diurnal Variation (Minute-Level Data)')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('BC (µg/m³)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('diurnal_variation_minute.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Diurnal variation from hourly data (separate figure)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for col in selected_columns:\n",
    "        if col in diurnal_pattern.columns:\n",
    "            plt.plot(diurnal_pattern.index, diurnal_pattern[col], label=f\"{col.split()[0]} ({wavelengths[col.split()[0]]} nm)\")\n",
    "    plt.title('Diurnal Variation (Hourly Data)')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('BC (µg/m³)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('diurnal_variation_hourly.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return diurnal_pattern, minute_diurnal\n",
    "\n",
    "def extract_original_data_issues(filepath):\n",
    "    print(\"\\nExtracting problematic data examples...\")\n",
    "    raw_df = pd.read_csv(filepath)\n",
    "    \n",
    "    problematic_rows = {}\n",
    "    for col in BC_COLUMNS:\n",
    "        if col in raw_df.columns:\n",
    "            temp_series = pd.to_numeric(raw_df[col], errors='coerce')\n",
    "            non_numeric_mask = temp_series.isna() & raw_df[col].notna()\n",
    "            if non_numeric_mask.sum() > 0:\n",
    "                key = f\"{col} (non-numeric)\"\n",
    "                problematic_rows[key] = raw_df.loc[non_numeric_mask].head(5)\n",
    "                sample_file = f\"problematic_data_{col.replace(' ', '_')}.csv\"\n",
    "                problematic_rows[key].to_csv(sample_file, index=False)\n",
    "                print(f\"Saved examples of non-numeric data from {col} to {sample_file}\")\n",
    "    return problematic_rows\n",
    "\n",
    "def full_analysis(filepath):\n",
    "    # Extract examples of problematic data\n",
    "    problematic_rows = extract_original_data_issues(filepath)\n",
    "    \n",
    "    # Analyze missing data and processing steps\n",
    "    raw_df, processed_df, hourly_df, merged_df, missing_periods, removed_data = analyze_missing_data(filepath)\n",
    "    \n",
    "    # Visualize missing data (graphs saved separately)\n",
    "    visualize_missing_data(merged_df, BC_COLUMNS, missing_periods, removed_data)\n",
    "    \n",
    "    # Explore alternative resampling methods\n",
    "    daily_df, two_hour_df, thirty_min_df, methods_comparison = alternative_resampling_methods(raw_df, BC_COLUMNS)\n",
    "    \n",
    "    # Calculate diurnal variations\n",
    "    diurnal_pattern, minute_diurnal = calculate_diurnal_variations(raw_df, BC_COLUMNS, WAVELENGTHS)\n",
    "    \n",
    "    # Create a simple report\n",
    "    hourly_completeness = merged_df[BC_COLUMNS].count() / len(merged_df)\n",
    "    daily_completeness_actual = daily_df[BC_COLUMNS].count() / len(daily_df)\n",
    "    \n",
    "    with open(\"missing_data_analysis_report.txt\", \"w\") as f:\n",
    "        f.write(\"BC Data Analysis Report\\n\")\n",
    "        f.write(\"======================\\n\\n\")\n",
    "        \n",
    "        f.write(\"1. DATA CLEANING SUMMARY\\n\")\n",
    "        f.write(\"----------------------\\n\")\n",
    "        f.write(f\"- Non-numeric values found: {sum(len(v) for v in removed_data['non_numeric_values'].values()) if removed_data['non_numeric_values'] else 0}\\n\")\n",
    "        f.write(f\"- Negative values removed: {sum(removed_data['negative_values'].values())}\\n\")\n",
    "        f.write(f\"- Outliers removed: {sum(removed_data['outliers'].values())}\\n\\n\")\n",
    "        \n",
    "        f.write(\"2. MISSING DATA ANALYSIS\\n\")\n",
    "        f.write(\"----------------------\\n\")\n",
    "        f.write(f\"- Total timespan: {merged_df.index.min()} to {merged_df.index.max()}\\n\")\n",
    "        f.write(f\"- Total hours in timespan: {len(merged_df)}\\n\")\n",
    "        completely_missing = merged_df[BC_COLUMNS].isnull().all(axis=1).sum()\n",
    "        f.write(f\"- Completely missing hours (all columns): {completely_missing} ({completely_missing/len(merged_df)*100:.1f}%)\\n\")\n",
    "        \n",
    "        f.write(\"\\n3. LONGEST MISSING PERIODS\\n\")\n",
    "        f.write(\"------------------------\\n\")\n",
    "        for i, period in enumerate(missing_periods[:5]):\n",
    "            f.write(f\"- Period {i+1}: {period['start']} to {period['end']} ({period['count']} hours)\\n\")\n",
    "        \n",
    "        f.write(\"\\n4. DATA COMPLETENESS BY METHOD\\n\")\n",
    "        f.write(\"----------------------------\\n\")\n",
    "        f.write(\"Hourly averages (original method):\\n\")\n",
    "        for col, pct in hourly_completeness.items():\n",
    "            f.write(f\"  - {col}: {pct*100:.1f}%\\n\")\n",
    "        \n",
    "        f.write(\"\\nDaily averages (direct method):\\n\")\n",
    "        for col, pct in daily_completeness_actual.items():\n",
    "            f.write(f\"  - {col}: {pct*100:.1f}%\\n\")\n",
    "        \n",
    "        f.write(\"\\n5. RECOMMENDATIONS\\n\")\n",
    "        f.write(\"-----------------------------------\\n\")\n",
    "        hourly_avg_completeness = hourly_completeness.mean()\n",
    "        daily_avg_completeness = daily_completeness_actual.mean()\n",
    "        \n",
    "        if daily_avg_completeness > hourly_avg_completeness:\n",
    "            f.write(\"1. Use direct aggregation from minute-level data to daily averages as suggested by Prof. Ann.\\n\")\n",
    "            f.write(f\"   - Data completeness improved from {hourly_avg_completeness*100:.1f}% to {daily_avg_completeness*100:.1f}%\\n\")\n",
    "        else:\n",
    "            f.write(\"1. Hourly averaging gives similar completeness to daily averaging.\\n\")\n",
    "        \n",
    "        f.write(\"2. For diurnal analysis, consider grouping minute-level data by hour directly to retain more data points.\\n\")\n",
    "        f.write(\"3. Note periods with completely missing data as potential limitations in the analysis.\\n\")\n",
    "    \n",
    "    print(\"Full analysis complete. Check generated visualizations and 'missing_data_analysis_report.txt'.\")\n",
    "    \n",
    "    return {\n",
    "        'raw_df': raw_df,\n",
    "        'processed_df': processed_df,\n",
    "        'hourly_df': hourly_df,\n",
    "        'merged_df': merged_df,\n",
    "        'missing_periods': missing_periods,\n",
    "        'removed_data': removed_data,\n",
    "        'daily_df': daily_df,\n",
    "        'two_hour_df': two_hour_df,\n",
    "        'thirty_min_df': thirty_min_df,\n",
    "        'methods_comparison': methods_comparison,\n",
    "        'diurnal_pattern': diurnal_pattern,\n",
    "        'minute_diurnal': minute_diurnal,\n",
    "        'problematic_rows': problematic_rows\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"/Users/ahmadjalil/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data/Aethelometry Data/Jacros_MA350_1-min_2022-2024_Cleaned.csv\"\n",
    "    results = full_analysis(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting problematic data examples...\n",
      "Loading raw data...\n",
      "\n",
      "Checking for non-numeric values in data columns...\n",
      "Warning: Sen and Ref columns not found, cannot apply Cai et al. algorithms\n",
      "\n",
      "Processing data...\n",
      "\n",
      "Negative values removed:\n",
      "UV BC1: 1116 values\n",
      "UV BC2: 2034 values\n",
      "UV BCc: 26701 values\n",
      "Blue BC1: 791 values\n",
      "Blue BC2: 657 values\n",
      "Blue BCc: 18394 values\n",
      "Green BC1: 1318 values\n",
      "Green BC2: 1072 values\n",
      "Green BCc: 19824 values\n",
      "Red BC1: 919 values\n",
      "Red BC2: 721 values\n",
      "Red BCc: 21969 values\n",
      "IR BC1: 1104 values\n",
      "IR BC2: 597 values\n",
      "IR BCc: 30983 values\n",
      "\n",
      "Outliers removed:\n",
      "UV BC1: 22312 values\n",
      "UV BC2: 22918 values\n",
      "UV BCc: 22942 values\n",
      "Blue BC1: 22512 values\n",
      "Blue BC2: 23093 values\n",
      "Blue BCc: 140 values\n",
      "Green BC1: 22646 values\n",
      "Green BC2: 23171 values\n",
      "Green BCc: 16717 values\n",
      "Red BC1: 22804 values\n",
      "Red BC2: 23329 values\n",
      "Red BCc: 21484 values\n",
      "IR BC1: 23150 values\n",
      "IR BC2: 23615 values\n",
      "IR BCc: 22771 values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/4q55gl35679357bs417130z40000gn/T/ipykernel_57980/1015572904.py:330: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hourly_df = processed_df.resample('H').mean(numeric_only=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing missing hours in the resampled data...\n",
      "Missing hourly values (out of 20665 total hours):\n",
      "UV BC1       2367\n",
      "UV BC2       2367\n",
      "UV BCc       2366\n",
      "Blue BC1     2363\n",
      "Blue BC2     2362\n",
      "Blue BCc     2340\n",
      "Green BC1    2363\n",
      "Green BC2    2366\n",
      "Green BCc    2352\n",
      "Red BC1      2365\n",
      "Red BC2      2368\n",
      "Red BCc      2352\n",
      "IR BC1       2367\n",
      "IR BC2       2368\n",
      "IR BCc       2345\n",
      "dtype: int64\n",
      "\n",
      "Completely missing hours (all columns): 2339\n",
      "\n",
      "Longest periods of completely missing data:\n",
      "1. From 2022-06-23 23:00:00+03:00 to 2022-07-27 10:00:00+03:00 (804 hours)\n",
      "2. From 2023-06-21 07:00:00+03:00 to 2023-06-29 09:00:00+03:00 (195 hours)\n",
      "3. From 2023-12-06 08:00:00+03:00 to 2023-12-12 09:00:00+03:00 (146 hours)\n",
      "4. From 2023-01-17 21:00:00+03:00 to 2023-01-23 04:00:00+03:00 (128 hours)\n",
      "5. From 2024-04-04 01:00:00+03:00 to 2024-04-08 20:00:00+03:00 (116 hours)\n",
      "6. From 2024-07-25 19:00:00+03:00 to 2024-07-30 10:00:00+03:00 (112 hours)\n",
      "7. From 2023-02-18 08:00:00+03:00 to 2023-02-22 06:00:00+03:00 (95 hours)\n",
      "8. From 2023-02-05 19:00:00+03:00 to 2023-02-09 05:00:00+03:00 (83 hours)\n",
      "9. From 2023-04-13 06:00:00+03:00 to 2023-04-15 09:00:00+03:00 (52 hours)\n",
      "10. From 2023-02-02 07:00:00+03:00 to 2023-02-03 20:00:00+03:00 (38 hours)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/4q55gl35679357bs417130z40000gn/T/ipykernel_57980/1015572904.py:336: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data visualizations saved as separate files.\n",
      "\n",
      "Exploring alternative resampling methods...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/4q55gl35679357bs417130z40000gn/T/ipykernel_57980/4167265038.py:163: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  two_hour_df = raw_df_copy[bc_columns].resample('2H').mean()\n",
      "/var/folders/q8/4q55gl35679357bs417130z40000gn/T/ipykernel_57980/4167265038.py:167: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  thirty_min_df = raw_df_copy[bc_columns].resample('30T').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data completeness comparison:\n",
      "              Daily    2-Hour  30-Minute\n",
      "UV BC1     0.922274  0.888416   0.885864\n",
      "UV BC2     0.922274  0.888416   0.885864\n",
      "UV BCc     0.922274  0.888416   0.885864\n",
      "Blue BC1   0.922274  0.888416   0.885864\n",
      "Blue BC2   0.922274  0.888416   0.885864\n",
      "Blue BCc   0.922274  0.888416   0.885864\n",
      "Green BC1  0.922274  0.888416   0.885864\n",
      "Green BC2  0.922274  0.888416   0.885864\n",
      "Green BCc  0.922274  0.888416   0.885864\n",
      "Red BC1    0.922274  0.888416   0.885864\n",
      "Red BC2    0.922274  0.888416   0.885864\n",
      "Red BCc    0.922274  0.888416   0.885864\n",
      "IR BC1     0.922274  0.888416   0.885864\n",
      "IR BC2     0.922274  0.888416   0.885864\n",
      "IR BCc     0.922274  0.888416   0.885864\n",
      "\n",
      "Calculating diurnal variations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/4q55gl35679357bs417130z40000gn/T/ipykernel_57980/4167265038.py:201: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hourly_df = df_copy[bc_columns].resample('H').mean(numeric_only=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced analysis complete. Check generated visualizations and 'enhanced_bc_analysis_report.txt'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define global constants for BC columns and wavelengths\n",
    "BC_COLUMNS = [\n",
    "    'UV BC1', 'UV BC2', 'UV BCc',\n",
    "    'Blue BC1', 'Blue BC2', 'Blue BCc',\n",
    "    'Green BC1', 'Green BC2', 'Green BCc',\n",
    "    'Red BC1', 'Red BC2', 'Red BCc',\n",
    "    'IR BC1', 'IR BC2', 'IR BCc'\n",
    "]\n",
    "\n",
    "# Add specific attenuation cross-section values from Cai et al. (2013)\n",
    "WAVELENGTHS = {\n",
    "    'UV': {'nm': 375, 'sigma_atn': 24.069}, \n",
    "    'Blue': {'nm': 470, 'sigma_atn': 19.070}, \n",
    "    'Green': {'nm': 528, 'sigma_atn': 17.028}, \n",
    "    'Red': {'nm': 625, 'sigma_atn': 14.091}, \n",
    "    'IR': {'nm': 880, 'sigma_atn': 10.120}\n",
    "}\n",
    "\n",
    "def apply_cai_algorithms(raw_df, flow_rate=50, time_base=5, threshold_a=None, threshold_b=75):\n",
    "    \"\"\"\n",
    "    Apply Cai et al. (2013) algorithms to identify suspect data points\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    raw_df : pandas DataFrame\n",
    "        DataFrame containing raw data with 'Sen' and 'Ref' columns\n",
    "    flow_rate : int\n",
    "        Flow rate in ml/min (default: 50)\n",
    "    time_base : int\n",
    "        Time base in minutes (default: 5)\n",
    "    threshold_a : int or None\n",
    "        Threshold for Algorithm 1 (default: None, will be set based on flow_rate and time_base)\n",
    "    threshold_b : int\n",
    "        Threshold for Algorithm 2 (default: 75, as per Cai et al.)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_with_flags : pandas DataFrame\n",
    "        Original DataFrame with added flag column\n",
    "    \"\"\"\n",
    "    print(\"\\nApplying Cai et al. (2013) algorithms to identify suspect data points...\")\n",
    "    \n",
    "    # Create a copy of the DataFrame\n",
    "    df = raw_df.copy()\n",
    "    \n",
    "    # Set default threshold_a based on flow_rate and time_base if not provided\n",
    "    if threshold_a is None:\n",
    "        if flow_rate == 50 and time_base == 5:\n",
    "            threshold_a = 750\n",
    "        elif flow_rate == 100 and time_base == 1:\n",
    "            threshold_a = 175\n",
    "        else:\n",
    "            # For other settings, use the 50 ml/min and 5 min setting as default\n",
    "            threshold_a = 750\n",
    "            print(f\"Warning: Using default threshold_a={threshold_a} for flow_rate={flow_rate} and time_base={time_base}\")\n",
    "    \n",
    "    # Check if 'Sen' and 'Ref' columns exist\n",
    "    if 'Sen' not in df.columns or 'Ref' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'Sen' and 'Ref' columns\")\n",
    "    \n",
    "    # Calculate ΔRef and ΔSen\n",
    "    df['ΔRef'] = df['Ref'].diff()\n",
    "    df['ΔSen'] = df['Sen'].diff()\n",
    "    \n",
    "    # Apply Algorithm 1: |ΔRef| > Threshold A\n",
    "    algo1_flag = abs(df['ΔRef']) > threshold_a\n",
    "    \n",
    "    # Apply Algorithm 2: (ΔSen - ΔRef) > Threshold B\n",
    "    algo2_flag = (df['ΔSen'] - df['ΔRef']) > threshold_b\n",
    "    \n",
    "    # Combine flags\n",
    "    df['flag'] = algo1_flag | algo2_flag\n",
    "    \n",
    "    # Flag the subsequent point as well (as per Cai et al.)\n",
    "    df['flag'] = df['flag'] | df['flag'].shift(-1).fillna(False)\n",
    "    \n",
    "    # Count flagged points\n",
    "    flagged_count = df['flag'].sum()\n",
    "    total_count = len(df)\n",
    "    flagged_percent = (flagged_count / total_count) * 100\n",
    "    \n",
    "    print(f\"Flagged {flagged_count} out of {total_count} data points ({flagged_percent:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_data_with_cai_methods(df_with_flags, option=1, bc_col='BC'):\n",
    "    \"\"\"\n",
    "    Apply data cleaning methods as per Cai et al. (2013)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_with_flags : pandas DataFrame\n",
    "        DataFrame with flag column from apply_cai_algorithms\n",
    "    option : int\n",
    "        Cleaning option (1, 2, or 3) as per Cai et al. (2013)\n",
    "    bc_col : str\n",
    "        Column name for BC data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cleaned_df : pandas DataFrame\n",
    "        DataFrame with cleaned BC data\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying data cleaning option {option}...\")\n",
    "    \n",
    "    # Create a copy of the DataFrame\n",
    "    df = df_with_flags.copy()\n",
    "    \n",
    "    # Create a new column for cleaned BC\n",
    "    cleaned_col = f\"{bc_col}_cleaned\"\n",
    "    df[cleaned_col] = df[bc_col].copy()\n",
    "    \n",
    "    # Get indices of flagged points\n",
    "    flagged_indices = df[df['flag']].index\n",
    "    \n",
    "    if option == 1:\n",
    "        # Option 1: Remove flagged data points\n",
    "        df.loc[flagged_indices, cleaned_col] = np.nan\n",
    "        print(f\"Option 1: Set {len(flagged_indices)} flagged points to NaN\")\n",
    "        \n",
    "    elif option == 2:\n",
    "        # Option 2: Interpolate between points before and after\n",
    "        for idx in flagged_indices:\n",
    "            # Find the nearest non-flagged points before and after\n",
    "            prev_idx = df.index[df.index < idx]\n",
    "            if len(prev_idx) > 0:\n",
    "                prev_idx = prev_idx[-1]\n",
    "                while prev_idx in flagged_indices and prev_idx > df.index[0]:\n",
    "                    temp_prev = df.index[df.index < prev_idx]\n",
    "                    if len(temp_prev) > 0:\n",
    "                        prev_idx = temp_prev[-1]\n",
    "                    else:\n",
    "                        break\n",
    "            \n",
    "            next_idx = df.index[df.index > idx]\n",
    "            if len(next_idx) > 0:\n",
    "                next_idx = next_idx[0]\n",
    "                while next_idx in flagged_indices and next_idx < df.index[-1]:\n",
    "                    temp_next = df.index[df.index > next_idx]\n",
    "                    if len(temp_next) > 0:\n",
    "                        next_idx = temp_next[0]\n",
    "                    else:\n",
    "                        break\n",
    "            \n",
    "            # If valid prev and next indices found, interpolate\n",
    "            if len(prev_idx) > 0 and len(next_idx) > 0 and prev_idx not in flagged_indices and next_idx not in flagged_indices:\n",
    "                df.loc[idx, cleaned_col] = (df.loc[prev_idx, bc_col] + df.loc[next_idx, bc_col]) / 2\n",
    "            else:\n",
    "                df.loc[idx, cleaned_col] = np.nan\n",
    "        \n",
    "        print(f\"Option 2: Interpolated {len(flagged_indices)} flagged points\")\n",
    "        \n",
    "    elif option == 3:\n",
    "        # Option 3: Physical model using ATN change\n",
    "        if 'ATN' not in df.columns:\n",
    "            print(\"Warning: ATN column not found, calculating from Sen/Ref\")\n",
    "            df['ATN'] = -100 * np.log(df['Sen'] / df['Ref'])\n",
    "        \n",
    "        for idx in flagged_indices:\n",
    "            # Find the nearest non-flagged points before and after\n",
    "            prev_idx = df.index[df.index < idx]\n",
    "            if len(prev_idx) > 0:\n",
    "                prev_idx = prev_idx[-1]\n",
    "                while prev_idx in flagged_indices and prev_idx > df.index[0]:\n",
    "                    temp_prev = df.index[df.index < prev_idx]\n",
    "                    if len(temp_prev) > 0:\n",
    "                        prev_idx = temp_prev[-1]\n",
    "                    else:\n",
    "                        break\n",
    "            \n",
    "            next_idx = df.index[df.index > idx]\n",
    "            if len(next_idx) > 0:\n",
    "                next_idx = next_idx[0]\n",
    "                while next_idx in flagged_indices and next_idx < df.index[-1]:\n",
    "                    temp_next = df.index[df.index > next_idx]\n",
    "                    if len(temp_next) > 0:\n",
    "                        next_idx = temp_next[0]\n",
    "                    else:\n",
    "                        break\n",
    "            \n",
    "            # If valid prev and next indices found, use ATN change\n",
    "            if len(prev_idx) > 0 and len(next_idx) > 0 and prev_idx not in flagged_indices and next_idx not in flagged_indices:\n",
    "                delta_atn = df.loc[next_idx, 'ATN'] - df.loc[prev_idx, 'ATN']\n",
    "                delta_t = (next_idx - prev_idx).total_seconds() / 60  # in minutes\n",
    "                \n",
    "                # Assuming WAVELENGTHS contains sigma_atn in m²/g for the wavelength\n",
    "                # Default to IR wavelength if specific wavelength not identifiable\n",
    "                sigma_atn = WAVELENGTHS.get('IR', {}).get('sigma_atn', 10.120)\n",
    "                \n",
    "                # Extract wavelength info from column name if possible\n",
    "                for wave_key in WAVELENGTHS:\n",
    "                    if wave_key in bc_col:\n",
    "                        sigma_atn = WAVELENGTHS[wave_key]['sigma_atn']\n",
    "                        break\n",
    "                \n",
    "                # Q is flow rate in m³/min\n",
    "                Q = flow_rate / 1000000  # Convert ml/min to m³/min\n",
    "                \n",
    "                # Calculate BC using physical model\n",
    "                if delta_t > 0:\n",
    "                    bc = delta_atn / (sigma_atn * Q * delta_t)\n",
    "                    df.loc[idx, cleaned_col] = bc\n",
    "                else:\n",
    "                    df.loc[idx, cleaned_col] = np.nan\n",
    "            else:\n",
    "                df.loc[idx, cleaned_col] = np.nan\n",
    "        \n",
    "        print(f\"Option 3: Applied physical model to {len(flagged_indices)} flagged points\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Option must be 1, 2, or 3\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_missing_data_enhanced(filepath, flow_rate=50, time_base=5):\n",
    "    \"\"\"\n",
    "    Enhanced version of your analyze_missing_data function that incorporates Cai et al. algorithms\n",
    "    \"\"\"\n",
    "    # Part 1: Load the raw data (same as your original function)\n",
    "    print(\"Loading raw data...\")\n",
    "    raw_df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Convert 'Time (UTC)' to datetime and then create local time\n",
    "    raw_df['Time (UTC)'] = pd.to_datetime(raw_df['Time (UTC)'], utc=True)\n",
    "    raw_df['Time (Local)'] = raw_df['Time (UTC)'].dt.tz_convert('Africa/Addis_Ababa')\n",
    "    \n",
    "    # Check for non-numeric values in BC columns and force conversion\n",
    "    print(\"\\nChecking for non-numeric values in data columns...\")\n",
    "    non_numeric_data = {}\n",
    "    for col in BC_COLUMNS:\n",
    "        if col in raw_df.columns:\n",
    "            converted = pd.to_numeric(raw_df[col], errors='coerce')\n",
    "            non_numeric_mask = converted.isna() & raw_df[col].notna()\n",
    "            non_numeric_count = non_numeric_mask.sum()\n",
    "            \n",
    "            if non_numeric_count > 0:\n",
    "                print(f\"WARNING: Found {non_numeric_count} non-numeric values in {col}\")\n",
    "                non_numeric_data[col] = raw_df.loc[non_numeric_mask, col].value_counts().to_dict()\n",
    "                print(f\"Example non-numeric values: {list(non_numeric_data[col].keys())[:3]}\")\n",
    "                \n",
    "            # Replace the column with its numeric conversion\n",
    "            raw_df[col] = converted\n",
    "    \n",
    "    # NEW: Apply Cai et al. algorithms if Sen and Ref columns exist\n",
    "    if 'Sen' in raw_df.columns and 'Ref' in raw_df.columns:\n",
    "        # Apply algorithms to each BC column\n",
    "        flagged_dfs = {}\n",
    "        for col in BC_COLUMNS:\n",
    "            if col in raw_df.columns:\n",
    "                # Create subset with just this BC column and Sen/Ref\n",
    "                subset_df = raw_df[['Time (Local)', col, 'Sen', 'Ref']].copy()\n",
    "                flagged_df = apply_cai_algorithms(subset_df, flow_rate=flow_rate, time_base=time_base)\n",
    "                \n",
    "                # Apply all three cleaning options for comparison\n",
    "                for option in [1, 2, 3]:\n",
    "                    cleaned_df = clean_data_with_cai_methods(flagged_df, option=option, bc_col=col)\n",
    "                    raw_df[f\"{col}_cleaned_opt{option}\"] = cleaned_df[f\"{col}_cleaned\"]\n",
    "                \n",
    "                flagged_dfs[col] = flagged_df\n",
    "        \n",
    "        # Create visualization to compare raw vs cleaned data\n",
    "        for col in BC_COLUMNS[:1]:  # Just do the first column for example\n",
    "            if col in raw_df.columns:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.plot(raw_df['Time (Local)'], raw_df[col], 'b-', alpha=0.5, label='Raw')\n",
    "                plt.plot(raw_df['Time (Local)'], raw_df[f\"{col}_cleaned_opt1\"], 'r-', label='Cleaned (Option 1)')\n",
    "                plt.plot(raw_df['Time (Local)'], raw_df[f\"{col}_cleaned_opt2\"], 'g-', label='Cleaned (Option 2)')\n",
    "                plt.plot(raw_df['Time (Local)'], raw_df[f\"{col}_cleaned_opt3\"], 'y-', label='Cleaned (Option 3)')\n",
    "                plt.title(f'Comparison of Raw vs Cleaned Data for {col}')\n",
    "                plt.xlabel('Time')\n",
    "                plt.ylabel('BC (µg/m³)')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'cleaned_data_comparison_{col.replace(\" \", \"_\")}.png')\n",
    "                plt.close()\n",
    "    else:\n",
    "        print(\"Warning: Sen and Ref columns not found, cannot apply Cai et al. algorithms\")\n",
    "    \n",
    "    # Continue with the rest of your original function...\n",
    "    # Part 2: Data processing steps\n",
    "    print(\"\\nProcessing data...\")\n",
    "    processed_df = raw_df.copy()\n",
    "    \n",
    "    removed_data = {\n",
    "        'non_numeric_values': non_numeric_data,\n",
    "        'negative_values': {},\n",
    "        'outliers': {}\n",
    "    }\n",
    "    \n",
    "    # Convert from ng/m³ to µg/m³ for BC columns\n",
    "    for col in BC_COLUMNS:\n",
    "        if col in processed_df.columns:\n",
    "            processed_df[col] = processed_df[col] / 1000\n",
    "        else:\n",
    "            print(f\"Warning: Column {col} not found - skipping\")\n",
    "    \n",
    "    # Remove negative values\n",
    "    for col in BC_COLUMNS:\n",
    "        if col in processed_df.columns:\n",
    "            neg_mask = processed_df[col] < 0\n",
    "            removed_data['negative_values'][col] = neg_mask.sum()\n",
    "            processed_df.loc[neg_mask, col] = np.nan\n",
    "    \n",
    "    # Remove outliers (values greater than mean + 3*std)\n",
    "    for col in BC_COLUMNS:\n",
    "        if col in processed_df.columns:\n",
    "            mean = processed_df[col].mean()\n",
    "            std = processed_df[col].std()\n",
    "            upper_limit = mean + 3 * std\n",
    "            outlier_mask = processed_df[col] > upper_limit\n",
    "            removed_data['outliers'][col] = outlier_mask.sum()\n",
    "            processed_df.loc[outlier_mask, col] = np.nan\n",
    "            \n",
    "    print(\"\\nNegative values removed:\")\n",
    "    for col, count in removed_data['negative_values'].items():\n",
    "        print(f\"{col}: {count} values\")\n",
    "    \n",
    "    print(\"\\nOutliers removed:\")\n",
    "    for col, count in removed_data['outliers'].items():\n",
    "        print(f\"{col}: {count} values\")\n",
    "    \n",
    "    # Part 3: Resample data to hourly averages\n",
    "    processed_df.set_index('Time (Local)', inplace=True)\n",
    "    hourly_df = processed_df.resample('H').mean(numeric_only=True)\n",
    "    \n",
    "    # Analyze missing hours in the resampled data\n",
    "    print(\"\\nAnalyzing missing hours in the resampled data...\")\n",
    "    start_date = hourly_df.index.min()\n",
    "    end_date = hourly_df.index.max()\n",
    "    full_date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "    full_df = pd.DataFrame(index=full_date_range)\n",
    "    merged_df = full_df.join(hourly_df)\n",
    "    \n",
    "    # Include cleaned columns in the analysis\n",
    "    all_bc_columns = BC_COLUMNS.copy()\n",
    "    for col in BC_COLUMNS:\n",
    "        for opt in [1, 2, 3]:\n",
    "            cleaned_col = f\"{col}_cleaned_opt{opt}\"\n",
    "            if cleaned_col in merged_df.columns:\n",
    "                all_bc_columns.append(cleaned_col)\n",
    "    \n",
    "    missing_hours = merged_df[all_bc_columns].isnull().sum()\n",
    "    print(f\"Missing hourly values (out of {len(full_date_range)} total hours):\")\n",
    "    print(missing_hours)\n",
    "    \n",
    "    completely_missing = merged_df[BC_COLUMNS].isnull().all(axis=1)\n",
    "    print(f\"\\nCompletely missing hours (all columns): {completely_missing.sum()}\")\n",
    "    \n",
    "    # Identify consecutive missing periods\n",
    "    missing_periods = []\n",
    "    current_period = None\n",
    "    for timestamp, is_missing in completely_missing.items():\n",
    "        if is_missing:\n",
    "            if current_period is None:\n",
    "                current_period = {\"start\": timestamp, \"end\": timestamp, \"count\": 1}\n",
    "            else:\n",
    "                current_period[\"end\"] = timestamp\n",
    "                current_period[\"count\"] += 1\n",
    "        else:\n",
    "            if current_period is not None:\n",
    "                missing_periods.append(current_period)\n",
    "                current_period = None\n",
    "    if current_period is not None:\n",
    "        missing_periods.append(current_period)\n",
    "    \n",
    "    missing_periods.sort(key=lambda x: x[\"count\"], reverse=True)\n",
    "    print(\"\\nLongest periods of completely missing data:\")\n",
    "    for i, period in enumerate(missing_periods[:10]):\n",
    "        print(f\"{i+1}. From {period['start']} to {period['end']} ({period['count']} hours)\")\n",
    "    \n",
    "    # Compare data completeness between raw and cleaned data\n",
    "    if 'Sen' in raw_df.columns and 'Ref' in raw_df.columns:\n",
    "        print(\"\\nComparing data completeness: Raw vs. Cleaned\")\n",
    "        for col in BC_COLUMNS:\n",
    "            if col in merged_df.columns:\n",
    "                raw_complete = 100 - (merged_df[col].isnull().sum() / len(merged_df) * 100)\n",
    "                for opt in [1, 2, 3]:\n",
    "                    cleaned_col = f\"{col}_cleaned_opt{opt}\"\n",
    "                    if cleaned_col in merged_df.columns:\n",
    "                        cleaned_complete = 100 - (merged_df[cleaned_col].isnull().sum() / len(merged_df) * 100)\n",
    "                        print(f\"{col}: Raw={raw_complete:.1f}%, Cleaned (Opt {opt})={cleaned_complete:.1f}%\")\n",
    "    \n",
    "    return raw_df, processed_df, hourly_df, merged_df, missing_periods, removed_data\n",
    "\n",
    "def full_analysis_enhanced(filepath, flow_rate=50, time_base=5):\n",
    "    \"\"\"\n",
    "    Enhanced version of your full_analysis function that incorporates Cai et al. algorithms\n",
    "    \"\"\"\n",
    "    # Extract examples of problematic data\n",
    "    problematic_rows = extract_original_data_issues(filepath)\n",
    "    \n",
    "    # Enhanced analyze_missing_data function\n",
    "    raw_df, processed_df, hourly_df, merged_df, missing_periods, removed_data = analyze_missing_data_enhanced(filepath, flow_rate, time_base)\n",
    "    \n",
    "    # Continue with your original function components...\n",
    "    # Visualize missing data\n",
    "    visualize_missing_data(merged_df, BC_COLUMNS, missing_periods, removed_data)\n",
    "    \n",
    "    # Explore alternative resampling methods\n",
    "    daily_df, two_hour_df, thirty_min_df, methods_comparison = alternative_resampling_methods(raw_df, BC_COLUMNS)\n",
    "    \n",
    "    # Calculate diurnal variations\n",
    "    wavelengths_nm = {k: v['nm'] for k, v in WAVELENGTHS.items()}\n",
    "    diurnal_pattern, minute_diurnal = calculate_diurnal_variations(raw_df, BC_COLUMNS, wavelengths_nm)\n",
    "    \n",
    "    # Create a simple report\n",
    "    hourly_completeness = merged_df[BC_COLUMNS].count() / len(merged_df)\n",
    "    daily_completeness_actual = daily_df[BC_COLUMNS].count() / len(daily_df)\n",
    "    \n",
    "    # Generate report including Cai et al. algorithm results\n",
    "    with open(\"enhanced_bc_analysis_report.txt\", \"w\") as f:\n",
    "        f.write(\"Enhanced BC Data Analysis Report with Cai et al. Algorithms\\n\")\n",
    "        f.write(\"====================================================\\n\\n\")\n",
    "        \n",
    "        f.write(\"1. DATA CLEANING SUMMARY\\n\")\n",
    "        f.write(\"----------------------\\n\")\n",
    "        f.write(f\"- Non-numeric values found: {sum(len(v) for v in removed_data['non_numeric_values'].values()) if removed_data['non_numeric_values'] else 0}\\n\")\n",
    "        f.write(f\"- Negative values removed: {sum(removed_data['negative_values'].values())}\\n\")\n",
    "        f.write(f\"- Outliers removed: {sum(removed_data['outliers'].values())}\\n\\n\")\n",
    "        \n",
    "        f.write(\"2. CAI ET AL. ALGORITHM RESULTS\\n\")\n",
    "        f.write(\"-----------------------------\\n\")\n",
    "        if 'Sen' in raw_df.columns and 'Ref' in raw_df.columns:\n",
    "            f.write(\"Applied Cai et al. (2013) algorithms for identifying suspect data points:\\n\")\n",
    "            f.write(\"- Algorithm 1: |ΔRef| > Threshold A (identifies abnormal reference signal fluctuations)\\n\")\n",
    "            f.write(\"- Algorithm 2: (ΔSen - ΔRef) > Threshold B (identifies when sensing deviates from reference)\\n\")\n",
    "            f.write(\"- Three data processing options were applied and compared\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"Could not apply Cai et al. algorithms - 'Sen' and 'Ref' columns not found in the data\\n\\n\")\n",
    "        \n",
    "        f.write(\"3. MISSING DATA ANALYSIS\\n\")\n",
    "        f.write(\"----------------------\\n\")\n",
    "        f.write(f\"- Total timespan: {merged_df.index.min()} to {merged_df.index.max()}\\n\")\n",
    "        f.write(f\"- Total hours in timespan: {len(merged_df)}\\n\")\n",
    "        completely_missing = merged_df[BC_COLUMNS].isnull().all(axis=1).sum()\n",
    "        f.write(f\"- Completely missing hours (all columns): {completely_missing} ({completely_missing/len(merged_df)*100:.1f}%)\\n\")\n",
    "        \n",
    "        f.write(\"\\n4. LONGEST MISSING PERIODS\\n\")\n",
    "        f.write(\"------------------------\\n\")\n",
    "        for i, period in enumerate(missing_periods[:5]):\n",
    "            f.write(f\"- Period {i+1}: {period['start']} to {period['end']} ({period['count']} hours)\\n\")\n",
    "        \n",
    "        f.write(\"\\n5. DATA COMPLETENESS BY METHOD\\n\")\n",
    "        f.write(\"----------------------------\\n\")\n",
    "        f.write(\"Hourly averages (original method):\\n\")\n",
    "        for col, pct in hourly_completeness.items():\n",
    "            f.write(f\"  - {col}: {pct*100:.1f}%\\n\")\n",
    "        \n",
    "        f.write(\"\\nDaily averages (direct method):\\n\")\n",
    "        for col, pct in daily_completeness_actual.items():\n",
    "            f.write(f\"  - {col}: {pct*100:.1f}%\\n\")\n",
    "        \n",
    "        f.write(\"\\n6. RECOMMENDATIONS\\n\")\n",
    "        f.write(\"-----------------------------------\\n\")\n",
    "        hourly_avg_completeness = hourly_completeness.mean()\n",
    "        daily_avg_completeness = daily_completeness_actual.mean()\n",
    "        \n",
    "        f.write(\"1. Implement Cai et al. (2013) algorithms for identifying suspect data points:\\n\")\n",
    "        f.write(\"   - Algorithm 1: |ΔRef| > Threshold A (where A ≈ 750 for 50 ml/min flow rate and 5 min time base)\\n\")\n",
    "        f.write(\"   - Algorithm 2: (ΔSen - ΔRef) > Threshold B (where B ≈ 75)\\n\")\n",
    "        \n",
    "        f.write(\"\\n2. For data cleaning, use Option 1 (removing flagged data) or Option 2 (interpolation) as they showed better reproducibility.\\n\")\n",
    "        \n",
    "        if daily_avg_completeness > hourly_avg_completeness:\n",
    "            f.write(\"\\n3. Use direct aggregation from minute-level data to daily averages as suggested by Prof. Ann.\\n\")\n",
    "            f.write(f\"   - Data completeness improved from {hourly_avg_completeness*100:.1f}% to {daily_avg_completeness*100:.1f}%\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n3. Hourly averaging gives similar completeness to daily averaging.\\n\")\n",
    "        \n",
    "        f.write(\"\\n4. For diurnal analysis, consider grouping minute-level data by hour directly to retain more data points.\\n\")\n",
    "        f.write(\"\\n5. Note periods with completely missing data as potential limitations in the analysis.\\n\")\n",
    "        \n",
    "        f.write(\"\\n6. When collecting new data, consider using diffusion driers if instruments will be used in environments with rapid humidity changes.\\n\")\n",
    "    \n",
    "    print(\"Enhanced analysis complete. Check generated visualizations and 'enhanced_bc_analysis_report.txt'.\")\n",
    "    \n",
    "    return {\n",
    "        'raw_df': raw_df,\n",
    "        'processed_df': processed_df,\n",
    "        'hourly_df': hourly_df,\n",
    "        'merged_df': merged_df,\n",
    "        'missing_periods': missing_periods,\n",
    "        'removed_data': removed_data,\n",
    "        'daily_df': daily_df,\n",
    "        'two_hour_df': two_hour_df,\n",
    "        'thirty_min_df': thirty_min_df,\n",
    "        'methods_comparison': methods_comparison,\n",
    "        'diurnal_pattern': diurnal_pattern,\n",
    "        'minute_diurnal': minute_diurnal,\n",
    "        'problematic_rows': problematic_rows\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"/Users/ahmadjalil/Library/CloudStorage/GoogleDrive-ahzs645@gmail.com/My Drive/University/Research/Grad/UC Davis Ann/NASA MAIA/Data/Aethelometry Data/Jacros_MA350_1-min_2022-2024_Cleaned.csv\"\n",
    "    results = full_analysis_enhanced(filepath, flow_rate=50, time_base=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
